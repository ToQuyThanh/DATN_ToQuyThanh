{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ae127a4-2d6e-4ef1-ba9a-26abf11aff16",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c8a9b55-760f-4119-a30f-a5596f822979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: natsort in /venv/main/lib/python3.12/site-packages (8.4.0)\n",
      "Requirement already satisfied: matplotlib in /venv/main/lib/python3.12/site-packages (3.10.6)\n",
      "Requirement already satisfied: thop in /venv/main/lib/python3.12/site-packages (0.1.1.post2209072238)\n",
      "Requirement already satisfied: torchsummary in /venv/main/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: tensorboardX in /venv/main/lib/python3.12/site-packages (2.6.4)\n",
      "Requirement already satisfied: colorlog in /venv/main/lib/python3.12/site-packages (6.9.0)\n",
      "Requirement already satisfied: pytorch_msssim in /venv/main/lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /venv/main/lib/python3.12/site-packages (from opencv-python) (2.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /venv/main/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /venv/main/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /venv/main/lib/python3.12/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /venv/main/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /venv/main/lib/python3.12/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /venv/main/lib/python3.12/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /venv/main/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: torch in /venv/main/lib/python3.12/site-packages (from thop) (2.8.0+cu129)\n",
      "Requirement already satisfied: protobuf>=3.20 in /venv/main/lib/python3.12/site-packages (from tensorboardX) (6.32.1)\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from torch->thop) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.12/site-packages (from torch->thop) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /venv/main/lib/python3.12/site-packages (from torch->thop) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /venv/main/lib/python3.12/site-packages (from torch->thop) (1.13.3)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.12/site-packages (from torch->thop) (3.3)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.12/site-packages (from torch->thop) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.12/site-packages (from torch->thop) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.9.86 in /venv/main/lib/python3.12/site-packages (from torch->thop) (12.9.86)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.9.79 in /venv/main/lib/python3.12/site-packages (from torch->thop) (12.9.79)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.9.79 in /venv/main/lib/python3.12/site-packages (from torch->thop) (12.9.79)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /venv/main/lib/python3.12/site-packages (from torch->thop) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.9.1.4 in /venv/main/lib/python3.12/site-packages (from torch->thop) (12.9.1.4)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.4.1.4 in /venv/main/lib/python3.12/site-packages (from torch->thop) (11.4.1.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.10.19 in /venv/main/lib/python3.12/site-packages (from torch->thop) (10.3.10.19)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.5.82 in /venv/main/lib/python3.12/site-packages (from torch->thop) (11.7.5.82)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.10.65 in /venv/main/lib/python3.12/site-packages (from torch->thop) (12.5.10.65)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /venv/main/lib/python3.12/site-packages (from torch->thop) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /venv/main/lib/python3.12/site-packages (from torch->thop) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.9.79 in /venv/main/lib/python3.12/site-packages (from torch->thop) (12.9.79)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.9.86 in /venv/main/lib/python3.12/site-packages (from torch->thop) (12.9.86)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.14.1.1 in /venv/main/lib/python3.12/site-packages (from torch->thop) (1.14.1.1)\n",
      "Requirement already satisfied: triton==3.4.0 in /venv/main/lib/python3.12/site-packages (from torch->thop) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.12/site-packages (from sympy>=1.13.3->torch->thop) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.12/site-packages (from jinja2->torch->thop) (2.1.5)\n",
      "Using cached opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.12.0.88\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python natsort matplotlib thop torchsummary tensorboardX colorlog pytorch_msssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87f357a4-7d92-4f89-b39c-65003601b654",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting natsort\n",
      "  Using cached natsort-8.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.6-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting thop\n",
      "  Using cached thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting torchsummary\n",
      "  Using cached torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Collecting tensorboardX\n",
      "  Using cached tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting colorlog\n",
      "  Using cached colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pytorch_msssim\n",
      "  Using cached pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.2.6)\n",
      "Collecting torch==2.8.0 (from torchvision)\n",
      "  Using cached torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "Collecting filelock (from torch==2.8.0->torchvision)\n",
      "  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from torch==2.8.0->torchvision) (68.1.2)\n",
      "Collecting sympy>=1.13.3 (from torch==2.8.0->torchvision)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.8.0->torchvision)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.1.6)\n",
      "Collecting fsspec (from torch==2.8.0->torchvision)\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.8.0->torchvision)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.8.0->torchvision)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.8.0->torchvision)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch==2.8.0->torchvision)\n",
      "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.8.0->torchvision)\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.8.0->torchvision)\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.8.0->torchvision)\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.8.0->torchvision)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.8.0->torchvision)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch==2.8.0->torchvision)\n",
      "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch==2.8.0->torchvision)\n",
      "  Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.8.0->torchvision)\n",
      "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.8.0->torchvision)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.8.0->torchvision)\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.4.0 (from torch==2.8.0->torchvision)\n",
      "  Using cached triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.60.0-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (111 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.12/dist-packages (from tensorboardX) (6.32.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch==2.8.0->torchvision)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->torchvision) (3.0.2)\n",
      "Using cached torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl (8.6 MB)\n",
      "Using cached torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
      "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python torchvision natsort matplotlib thop torchsummary tensorboardX colorlog pytorch_msssim natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa52ff8d-6e9b-43c4-8f6d-84c897a7c68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu129\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "import natsort\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0443559-4d30-4f27-9cfd-2c6ea6c0c322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 5090\n"
     ]
    }
   ],
   "source": [
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45428396-2c0d-4a82-87bd-d5635d984bcf",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b451c1e4-5422-4ae7-8396-aa16dac3ec8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available!\n",
      "Number of GPUs: 1\n",
      "GPU 0: NVIDIA GeForce RTX 5090\n",
      "Memory Allocated: 0.00 MB\n",
      "Memory Reserved: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Ki·ªÉm tra GPU c√≥ kh·∫£ d·ª•ng kh√¥ng\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"Memory Allocated: {torch.cuda.memory_allocated(i)/1024**2:.2f} MB\")\n",
    "        print(f\"Memory Reserved: {torch.cuda.memory_reserved(i)/1024**2:.2f} MB\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22df0e94-65c1-4a3e-8401-36361a367ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU cores: 192\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "print(\"CPU cores:\", multiprocessing.cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e618dc5-735c-450d-a767-68df828cc4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 5090\n",
      "T·ªïng b·ªô nh·ªõ GPU: 33.67 GB\n",
      "ƒê√£ c·∫•p ph√°t: 0.00 GB, Reserved: 0.00 GB, Free: 33.67 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Ki·ªÉm tra GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    total_mem = torch.cuda.get_device_properties(device).total_memory\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(device)}\")\n",
    "    print(f\"T·ªïng b·ªô nh·ªõ GPU: {total_mem / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Th√¥ng tin hi·ªán c√≥\n",
    "    reserved = torch.cuda.memory_reserved(device) / 1e9\n",
    "    allocated = torch.cuda.memory_allocated(device) / 1e9\n",
    "    free_mem = total_mem / 1e9 - reserved - allocated\n",
    "    print(f\"ƒê√£ c·∫•p ph√°t: {allocated:.2f} GB, Reserved: {reserved:.2f} GB, Free: {free_mem:.2f} GB\")\n",
    "else:\n",
    "    print(\"Kh√¥ng c√≥ GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bd78bc-c86d-4fd8-b66a-91dde413c2e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAINING START ===\n",
      "PyTorch: 2.8.0+cu129 | CUDA: True\n",
      "Epochs: 100 | Batch: 8 | LR: 0.0001\n",
      "Checking folder path: train_gen/images\n",
      "Checking folder path: test_gen/images\n",
      "Train: 5000 | Val: 1002\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2018/6/11 15:54\n",
    "# @Author  : zhoujun\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from torchvision import transforms\n",
    "from invoice_dataset import ImageData\n",
    "from docunet_model import TinyDocUnet\n",
    "import time\n",
    "import config\n",
    "from tensorboardX import SummaryWriter\n",
    "from docunet_loss import DocUnetLoss_DL_batch as DocUnetLoss\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from torch.amp import GradScaler, autocast\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from eval_scores import evaluate_batch\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def save_checkpoint(checkpoint_path, model, optimizer, epoch, scaler=None, metrics=None):\n",
    "    \"\"\"Save checkpoint - simplified\"\"\"\n",
    "    state = {\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'metrics': metrics or {}\n",
    "    }\n",
    "    if scaler is not None:\n",
    "        state['scaler'] = scaler.state_dict()\n",
    "    torch.save(state, checkpoint_path)\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer, scaler=None):\n",
    "    \"\"\"Load checkpoint\"\"\"\n",
    "    try:\n",
    "        state = torch.load(checkpoint_path, map_location='cpu')\n",
    "        model.load_state_dict(state['state_dict'])\n",
    "        start_epoch = state['epoch']\n",
    "        metrics = state.get('metrics', {})\n",
    "        if scaler is not None and 'scaler' in state:\n",
    "            scaler.load_state_dict(state['scaler'])\n",
    "        print(f'Loaded checkpoint from epoch {start_epoch}')\n",
    "        return start_epoch, metrics\n",
    "    except Exception as e:\n",
    "        print(f'Error loading checkpoint: {e}')\n",
    "        return 0, {}\n",
    "\n",
    "def validate_epoch(net, val_loader, criterion, device, use_amp=True):\n",
    "    \"\"\"Validation - t·ªëi ∆∞u, ch·ªâ t√≠nh to√°n metrics c·∫ßn thi·∫øt\"\"\"\n",
    "    net.eval()\n",
    "    total_ms_ssim = 0\n",
    "    total_ad = 0\n",
    "    total_val_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            try:\n",
    "                images, targets = images.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "                \n",
    "                if use_amp and torch.cuda.is_available():\n",
    "                    with autocast(device_type='cuda'):\n",
    "                        _, outputs = net(images)\n",
    "                        val_loss = criterion(outputs, targets)\n",
    "                    outputs = outputs.float()\n",
    "                    val_loss = val_loss.float()\n",
    "                else:\n",
    "                    _, outputs = net(images)\n",
    "                    val_loss = criterion(outputs, targets)\n",
    "                \n",
    "                targets = targets.float()\n",
    "                \n",
    "                if not torch.isfinite(val_loss):\n",
    "                    continue\n",
    "                \n",
    "                total_val_loss += val_loss.item()\n",
    "                \n",
    "                # Evaluate batch\n",
    "                ms_ssim_score, ad_score = evaluate_batch(outputs, targets)\n",
    "                \n",
    "                if not (np.isfinite(ms_ssim_score) and np.isfinite(ad_score)):\n",
    "                    continue\n",
    "                \n",
    "                total_ms_ssim += ms_ssim_score\n",
    "                total_ad += ad_score\n",
    "                num_batches += 1\n",
    "                \n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    if num_batches == 0:\n",
    "        return float('inf'), 0.0, float('inf')\n",
    "    \n",
    "    return total_val_loss / num_batches, total_ms_ssim / num_batches, total_ad / num_batches\n",
    "\n",
    "class TrainingMetrics:\n",
    "    \"\"\"Minimal metrics tracking\"\"\"\n",
    "    def __init__(self, output_dir):\n",
    "        self.output_dir = output_dir\n",
    "        self.metrics = defaultdict(list)\n",
    "        self.best_metrics = {\n",
    "            'best_train_loss': float('inf'),\n",
    "            'best_train_loss_epoch': 0,\n",
    "            'best_val_loss': float('inf'),\n",
    "            'best_val_loss_epoch': 0,\n",
    "            'best_ms_ssim': 0.0,\n",
    "            'best_ms_ssim_epoch': 0,\n",
    "            'best_ad': float('inf'),\n",
    "            'best_ad_epoch': 0,\n",
    "        }\n",
    "        \n",
    "    def update(self, epoch, **kwargs):\n",
    "        \"\"\"Update metrics\"\"\"\n",
    "        self.metrics['epoch'].append(epoch)\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and np.isfinite(value):\n",
    "                self.metrics[key].append(value)\n",
    "            \n",
    "        # Update best metrics\n",
    "        if 'train_loss' in kwargs and kwargs['train_loss'] is not None:\n",
    "            if kwargs['train_loss'] < self.best_metrics['best_train_loss']:\n",
    "                self.best_metrics['best_train_loss'] = kwargs['train_loss']\n",
    "                self.best_metrics['best_train_loss_epoch'] = epoch\n",
    "                \n",
    "        if 'val_loss' in kwargs and kwargs['val_loss'] is not None:\n",
    "            if kwargs['val_loss'] < self.best_metrics['best_val_loss']:\n",
    "                self.best_metrics['best_val_loss'] = kwargs['val_loss']\n",
    "                self.best_metrics['best_val_loss_epoch'] = epoch\n",
    "                \n",
    "        if 'val_ms_ssim' in kwargs and kwargs['val_ms_ssim'] is not None:\n",
    "            if kwargs['val_ms_ssim'] > self.best_metrics['best_ms_ssim']:\n",
    "                self.best_metrics['best_ms_ssim'] = kwargs['val_ms_ssim']\n",
    "                self.best_metrics['best_ms_ssim_epoch'] = epoch\n",
    "                \n",
    "        if 'val_ad' in kwargs and kwargs['val_ad'] is not None:\n",
    "            if kwargs['val_ad'] < self.best_metrics['best_ad']:\n",
    "                self.best_metrics['best_ad'] = kwargs['val_ad']\n",
    "                self.best_metrics['best_ad_epoch'] = epoch\n",
    "    \n",
    "    def save_metrics(self):\n",
    "        \"\"\"Save metrics to JSON\"\"\"\n",
    "        metrics_file = os.path.join(self.output_dir, 'training_metrics.json')\n",
    "        all_metrics = {\n",
    "            'training_history': dict(self.metrics),\n",
    "            'best_metrics': self.best_metrics,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        with open(metrics_file, 'w') as f:\n",
    "            json.dump(all_metrics, f, indent=2)\n",
    "    \n",
    "    def plot_training_curves(self):\n",
    "        \"\"\"T·∫°o training curves - ch·ªâ g·ªçi khi c·∫ßn\"\"\"\n",
    "        try:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "            \n",
    "            # Training vs Validation loss\n",
    "            if 'train_loss' in self.metrics:\n",
    "                axes[0,0].plot(self.metrics['epoch'], self.metrics['train_loss'], label='Train', color='blue')\n",
    "                if 'val_loss' in self.metrics:\n",
    "                    val_epochs = [self.metrics['epoch'][i] for i in range(len(self.metrics['val_loss'])) \n",
    "                                 if self.metrics['val_loss'][i] is not None]\n",
    "                    val_values = [v for v in self.metrics['val_loss'] if v is not None]\n",
    "                    if val_values:\n",
    "                        axes[0,0].plot(val_epochs, val_values, label='Val', color='red')\n",
    "                axes[0,0].set_title('Loss')\n",
    "                axes[0,0].set_xlabel('Epoch')\n",
    "                axes[0,0].set_ylabel('Loss')\n",
    "                axes[0,0].legend()\n",
    "                axes[0,0].grid(True)\n",
    "            \n",
    "            # Learning rate\n",
    "            if 'learning_rate' in self.metrics:\n",
    "                axes[0,1].plot(self.metrics['epoch'], self.metrics['learning_rate'])\n",
    "                axes[0,1].set_title('Learning Rate')\n",
    "                axes[0,1].set_xlabel('Epoch')\n",
    "                axes[0,1].set_yscale('log')\n",
    "                axes[0,1].grid(True)\n",
    "            \n",
    "            # MS-SSIM\n",
    "            if 'val_ms_ssim' in self.metrics:\n",
    "                epochs = [self.metrics['epoch'][i] for i in range(len(self.metrics['val_ms_ssim'])) \n",
    "                         if self.metrics['val_ms_ssim'][i] is not None]\n",
    "                values = [v for v in self.metrics['val_ms_ssim'] if v is not None]\n",
    "                if values:\n",
    "                    axes[1,0].plot(epochs, values, color='green')\n",
    "                    axes[1,0].set_title('MS-SSIM')\n",
    "                    axes[1,0].set_xlabel('Epoch')\n",
    "                    axes[1,0].grid(True)\n",
    "            \n",
    "            # AD\n",
    "            if 'val_ad' in self.metrics:\n",
    "                epochs = [self.metrics['epoch'][i] for i in range(len(self.metrics['val_ad'])) \n",
    "                         if self.metrics['val_ad'][i] is not None]\n",
    "                values = [v for v in self.metrics['val_ad'] if v is not None]\n",
    "                if values:\n",
    "                    axes[1,1].plot(epochs, values, color='orange')\n",
    "                    axes[1,1].set_title('AD')\n",
    "                    axes[1,1].set_xlabel('Epoch')\n",
    "                    axes[1,1].grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.output_dir, 'training_curves.png'), dpi=150)\n",
    "            plt.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def train():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(config.gpu_id)\n",
    "    if config.output_dir is None:\n",
    "        config.output_dir = 'output'\n",
    "    if config.restart_training:\n",
    "        shutil.rmtree(config.output_dir, ignore_errors=True)\n",
    "    if not os.path.exists(config.output_dir):\n",
    "        os.mkdir(config.output_dir)\n",
    "\n",
    "    metrics_tracker = TrainingMetrics(config.output_dir)\n",
    "    \n",
    "    # Minimal system info\n",
    "    print(\"=== TRAINING START ===\")\n",
    "    print(f\"PyTorch: {torch.__version__} | CUDA: {torch.cuda.is_available()}\")\n",
    "    print(f\"Epochs: {config.epochs} | Batch: {config.train_batch_size} | LR: {config.lr}\")\n",
    "    \n",
    "    # Device setup\n",
    "    if config.gpu_id is not None and torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        torch.manual_seed(config.seed)\n",
    "        torch.cuda.manual_seed_all(config.seed)\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        torch.manual_seed(config.seed)\n",
    "\n",
    "    # Data loading\n",
    "    train_data = ImageData(config.trainroot, transform=transforms.ToTensor(), t_transform=transforms.ToTensor())\n",
    "    train_loader = Data.DataLoader(\n",
    "        dataset=train_data, \n",
    "        batch_size=config.train_batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=int(config.workers),\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if int(config.workers) > 0 else False\n",
    "    )\n",
    "    \n",
    "    test_data = ImageData(config.testroot, transform=transforms.ToTensor(), t_transform=transforms.ToTensor())\n",
    "    test_loader = Data.DataLoader(\n",
    "        dataset=test_data, \n",
    "        batch_size=1, \n",
    "        shuffle=False, \n",
    "        num_workers=3,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {len(train_data)} | Val: {len(test_data)}\")\n",
    "\n",
    "    # Model setup\n",
    "    writer = SummaryWriter(config.output_dir)\n",
    "    net = TinyDocUnet(input_channels=3, n_classes=2).to(device)\n",
    "    criterion = DocUnetLoss(reduction='mean')\n",
    "    optimizer = torch.optim.AdamW(net.parameters(), lr=config.lr, weight_decay=1e-4)\n",
    "    \n",
    "    # Mixed precision\n",
    "    use_amp = getattr(config, 'use_amp', True) and torch.cuda.is_available()\n",
    "    scaler = GradScaler() if use_amp else None\n",
    "    grad_clip = getattr(config, 'grad_clip', 1.0)\n",
    "    accumulation_steps = getattr(config, 'accumulation_steps', 1)\n",
    "\n",
    "    # Load checkpoint\n",
    "    if config.checkpoint != '' and not config.restart_training:\n",
    "        start_epoch, _ = load_checkpoint(config.checkpoint, net, optimizer, scaler)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=start_epoch\n",
    "        )\n",
    "    else:\n",
    "        start_epoch = config.start_epoch\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=start_epoch - 1\n",
    "        )\n",
    "\n",
    "    all_step = len(train_loader)\n",
    "    global_step = start_epoch * all_step\n",
    "    \n",
    "    try:\n",
    "        training_start = time.time()\n",
    "        \n",
    "        for epoch in range(start_epoch, config.epochs):\n",
    "            net.train()\n",
    "            train_loss = 0.\n",
    "            accumulated_loss = 0.\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "                images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                \n",
    "                # Forward\n",
    "                if use_amp:\n",
    "                    with autocast(device_type='cuda'):\n",
    "                        _, y = net(images)\n",
    "                        loss = criterion(y, labels) / accumulation_steps\n",
    "                else:\n",
    "                    _, y = net(images)\n",
    "                    loss = criterion(y, labels) / accumulation_steps\n",
    "                \n",
    "                # Backward\n",
    "                if use_amp:\n",
    "                    scaler.scale(loss).backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                \n",
    "                accumulated_loss += loss.item()\n",
    "                train_loss += loss.item() * accumulation_steps\n",
    "                \n",
    "                # Update weights\n",
    "                if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "                    if use_amp:\n",
    "                        if grad_clip > 0:\n",
    "                            scaler.unscale_(optimizer)\n",
    "                            torch.nn.utils.clip_grad_norm_(net.parameters(), grad_clip)\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        if grad_clip > 0:\n",
    "                            torch.nn.utils.clip_grad_norm_(net.parameters(), grad_clip)\n",
    "                        optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    accumulated_loss = 0.\n",
    "                \n",
    "                global_step += 1\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            # Epoch summary - ch·ªâ log m·ªói 5 epoch\n",
    "            epoch_time = time.time() - epoch_start\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            \n",
    "            # Validation - m·ªói 5 epoch\n",
    "            val_loss, val_ms_ssim, val_ad = None, None, None\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                val_loss, val_ms_ssim, val_ad = validate_epoch(net, test_loader, criterion, device, use_amp)\n",
    "                \n",
    "                # Save best models\n",
    "                if val_loss < metrics_tracker.best_metrics['best_val_loss']:\n",
    "                    save_checkpoint(f'{config.output_dir}/best_val_loss.pth', net, optimizer, epoch + 1, scaler,\n",
    "                                  {'train_loss': avg_train_loss, 'val_loss': val_loss, 'ms_ssim': val_ms_ssim, 'ad': val_ad})\n",
    "                \n",
    "                if val_ms_ssim > metrics_tracker.best_metrics['best_ms_ssim']:\n",
    "                    save_checkpoint(f'{config.output_dir}/best_ms_ssim.pth', net, optimizer, epoch + 1, scaler,\n",
    "                                  {'train_loss': avg_train_loss, 'val_loss': val_loss, 'ms_ssim': val_ms_ssim, 'ad': val_ad})\n",
    "                \n",
    "                if val_ad < metrics_tracker.best_metrics['best_ad']:\n",
    "                    save_checkpoint(f'{config.output_dir}/best_ad.pth', net, optimizer, epoch + 1, scaler,\n",
    "                                  {'train_loss': avg_train_loss, 'val_loss': val_loss, 'ms_ssim': val_ms_ssim, 'ad': val_ad})\n",
    "                \n",
    "                # Print summary m·ªói 5 epoch\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"Epoch [{epoch+1}/{config.epochs}] - Time: {epoch_time:.1f}s\")\n",
    "                print(f\"Train Loss: {avg_train_loss:.6f} | LR: {current_lr:.2e}\")\n",
    "                print(f\"Val Loss: {val_loss:.6f} | MS-SSIM: {val_ms_ssim:.4f} | AD: {val_ad:.4f}\")\n",
    "                print(f\"Best - ValLoss: {metrics_tracker.best_metrics['best_val_loss']:.6f} \"\n",
    "                      f\"(E{metrics_tracker.best_metrics['best_val_loss_epoch']}) | \"\n",
    "                      f\"MS-SSIM: {metrics_tracker.best_metrics['best_ms_ssim']:.4f} \"\n",
    "                      f\"(E{metrics_tracker.best_metrics['best_ms_ssim_epoch']})\")\n",
    "                print(f\"{'='*60}\\n\")\n",
    "                \n",
    "                # TensorBoard - minimal logging\n",
    "                writer.add_scalar('Val/Loss', val_loss, epoch)\n",
    "                writer.add_scalar('Val/MS-SSIM', val_ms_ssim, epoch)\n",
    "                writer.add_scalar('Val/AD', val_ad, epoch)\n",
    "            else:\n",
    "                # Ch·ªâ print progress ng·∫Øn g·ªçn\n",
    "                print(f\"E{epoch+1:03d}/{config.epochs} | Loss: {avg_train_loss:.6f} | Time: {epoch_time:.1f}s\", end='\\r')\n",
    "            \n",
    "            # Update metrics\n",
    "            metrics_tracker.update(\n",
    "                epoch=epoch + 1,\n",
    "                train_loss=avg_train_loss,\n",
    "                val_loss=val_loss,\n",
    "                learning_rate=current_lr,\n",
    "                val_ms_ssim=val_ms_ssim,\n",
    "                val_ad=val_ad\n",
    "            )\n",
    "            \n",
    "            # TensorBoard - minimal\n",
    "            writer.add_scalar('Train/loss', avg_train_loss, epoch)\n",
    "            writer.add_scalar('Train/lr', current_lr, epoch)\n",
    "            \n",
    "            # Save checkpoint m·ªói 10 epoch\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                save_checkpoint(f'{config.output_dir}/checkpoint_e{epoch+1:03d}.pth', \n",
    "                              net, optimizer, epoch + 1, scaler,\n",
    "                              {'train_loss': avg_train_loss, 'val_loss': val_loss})\n",
    "            \n",
    "            # Save metrics & plots m·ªói 10 epoch\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                metrics_tracker.save_metrics()\n",
    "                metrics_tracker.plot_training_curves()\n",
    "            \n",
    "            # Memory cleanup\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Training completed\n",
    "        total_time = time.time() - training_start\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"üéâ TRAINING COMPLETED!\")\n",
    "        print(f\"Total time: {total_time/3600:.2f}h | Avg/epoch: {total_time/config.epochs:.1f}s\")\n",
    "        print(f\"Best Val Loss: {metrics_tracker.best_metrics['best_val_loss']:.6f} (E{metrics_tracker.best_metrics['best_val_loss_epoch']})\")\n",
    "        print(f\"Best MS-SSIM: {metrics_tracker.best_metrics['best_ms_ssim']:.4f} (E{metrics_tracker.best_metrics['best_ms_ssim_epoch']})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        metrics_tracker.save_metrics()\n",
    "        metrics_tracker.plot_training_curves()\n",
    "        save_checkpoint(f'{config.output_dir}/final_model.pth', net, optimizer, epoch + 1, scaler)\n",
    "        writer.close()\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚ö†Ô∏è Training interrupted\")\n",
    "        save_checkpoint(f'{config.output_dir}/interrupted_e{epoch+1}.pth', net, optimizer, epoch + 1, scaler)\n",
    "        metrics_tracker.save_metrics()\n",
    "        writer.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {e}\")\n",
    "        save_checkpoint(f'{config.output_dir}/error_e{epoch+1}.pth', net, optimizer, epoch + 1, scaler)\n",
    "        metrics_tracker.save_metrics()\n",
    "        writer.close()\n",
    "        raise\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a65c51f-478a-428e-b7e1-2259a9a5a4bd",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a1bf80-bbb8-4a7d-90f4-702694fdfde1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2018/6/11 15:54\n",
    "# @Author  : zhoujun\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from torchvision import transforms\n",
    "from invoice_dataset import ImageData\n",
    "from docunet_model_c import TinyDocUnet\n",
    "import time\n",
    "import config\n",
    "from tensorboardX import SummaryWriter\n",
    "from docunet_loss import DocUnetLoss_DL_batch as DocUnetLoss\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from torch.amp import GradScaler, autocast\n",
    "import logging\n",
    "from colorlog import ColoredFormatter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from eval_scores import evaluate_batch\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch._inductor.config.debug = 1\n",
    "\n",
    "def setup_logger(log_file_path: str = None):\n",
    "    \"\"\"Setup logger v·ªõi file v√† console output\"\"\"\n",
    "    # Clear existing handlers\n",
    "    logger = logging.getLogger('project')\n",
    "    logger.handlers.clear()\n",
    "    \n",
    "    # File handler\n",
    "    if log_file_path:\n",
    "        file_handler = logging.FileHandler(log_file_path)\n",
    "        file_formatter = logging.Formatter(\n",
    "            '%(asctime)s %(levelname)-8s %(filename)s: %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "        file_handler.setFormatter(file_formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "    \n",
    "    # Console handler v·ªõi m√†u\n",
    "    console_formatter = ColoredFormatter(\n",
    "        \"%(asctime)s %(log_color)s%(levelname)-8s %(reset)s %(filename)s: %(message)s\",\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        reset=True,\n",
    "        log_colors={\n",
    "            'DEBUG': 'blue',\n",
    "            'INFO': 'green', \n",
    "            'WARNING': 'yellow',\n",
    "            'ERROR': 'red',\n",
    "            'CRITICAL': 'red',\n",
    "        }\n",
    "    )\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.info('Logger initialized successfully')\n",
    "    return logger\n",
    "\n",
    "def save_checkpoint(checkpoint_path, model, optimizer, epoch, scaler=None, metrics=None):\n",
    "    \"\"\"Save checkpoint v·ªõi metadata chi ti·∫øt\"\"\"\n",
    "    state = {\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'metrics': metrics or {},\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'pytorch_version': torch.__version__\n",
    "    }\n",
    "    if scaler is not None:\n",
    "        state['scaler'] = scaler.state_dict()\n",
    "    \n",
    "    torch.save(state, checkpoint_path)\n",
    "    print(f'Model saved to {checkpoint_path}')\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer, scaler=None):\n",
    "    \"\"\"Load checkpoint v·ªõi error handling\"\"\"\n",
    "    try:\n",
    "        state = torch.load(checkpoint_path, map_location='cpu')\n",
    "        model.load_state_dict(state['state_dict'])\n",
    "        # Kh√¥ng load optimizer state ƒë·ªÉ tr√°nh conflict\n",
    "        # optimizer.load_state_dict(state['optimizer']) \n",
    "        start_epoch = state['epoch']\n",
    "        metrics = state.get('metrics', {})\n",
    "        \n",
    "        if scaler is not None and 'scaler' in state:\n",
    "            scaler.load_state_dict(state['scaler'])\n",
    "        \n",
    "        print(f'Model loaded from {checkpoint_path}')\n",
    "        print(f'Checkpoint timestamp: {state.get(\"timestamp\", \"Unknown\")}')\n",
    "        return start_epoch, metrics\n",
    "    except Exception as e:\n",
    "        print(f'Error loading checkpoint: {e}')\n",
    "        return 0, {}\n",
    "\n",
    "def validate_epoch(net, val_loader, criterion, device, logger, use_amp=True):\n",
    "    \"\"\"Validation v·ªõi AMP support, validation loss v√† error handling\"\"\"\n",
    "    net.eval()\n",
    "    total_ms_ssim = 0\n",
    "    total_ad = 0\n",
    "    total_val_loss = 0\n",
    "    num_batches = 0\n",
    "    val_start_time = time.time()\n",
    "    \n",
    "    validation_errors = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets) in enumerate(val_loader):\n",
    "            try:\n",
    "                # torch.compiler.cudagraph_mark_step_begin()\n",
    "                images, targets = images.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "                \n",
    "                # Forward pass v·ªõi mixed precision\n",
    "                if use_amp and torch.cuda.is_available():\n",
    "                    with autocast(device_type='cuda'):\n",
    "                        _, outputs = net(images)\n",
    "                        # Calculate validation loss\n",
    "                        val_loss = criterion(outputs, targets)\n",
    "                    # QUAN TR·ªåNG: Convert t·ª´ half precision v·ªÅ float32 cho evaluation\n",
    "                    outputs = outputs.float()\n",
    "                    val_loss = val_loss.float()\n",
    "                else:\n",
    "                    _, outputs = net(images)\n",
    "                    val_loss = criterion(outputs, targets)\n",
    "                \n",
    "                # ƒê·∫£m b·∫£o targets c≈©ng l√† float32\n",
    "                targets = targets.float()\n",
    "                \n",
    "                # Validate loss\n",
    "                if not torch.isfinite(val_loss):\n",
    "                    logger.warning(f'Invalid loss at validation batch {batch_idx}: {val_loss.item()}')\n",
    "                    validation_errors += 1\n",
    "                    continue\n",
    "                \n",
    "                # Accumulate validation loss\n",
    "                total_val_loss += val_loss.item()\n",
    "                \n",
    "                # Evaluate batch\n",
    "                ms_ssim_score, ad_score = evaluate_batch(outputs, targets)\n",
    "                \n",
    "                # Validate scores\n",
    "                if not (np.isfinite(ms_ssim_score) and np.isfinite(ad_score)):\n",
    "                    logger.warning(f'Invalid scores at batch {batch_idx}: MS-SSIM={ms_ssim_score}, AD={ad_score}')\n",
    "                    validation_errors += 1\n",
    "                    continue\n",
    "                \n",
    "                total_ms_ssim += ms_ssim_score\n",
    "                total_ad += ad_score\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Log progress\n",
    "                if (batch_idx + 1) % 20 == 0:\n",
    "                    logger.info(f'Validation [{batch_idx + 1}/{len(val_loader)}] - '\n",
    "                              f'Loss: {val_loss.item():.4f}, MS-SSIM: {ms_ssim_score:.4f}, AD: {ad_score:.4f}')\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f'Error in validation batch {batch_idx}: {str(e)}')\n",
    "                validation_errors += 1\n",
    "                continue\n",
    "    \n",
    "    if num_batches == 0:\n",
    "        logger.error(\"No valid batches in validation!\")\n",
    "        return float('inf'), 0.0, float('inf')\n",
    "    \n",
    "    avg_val_loss = total_val_loss / num_batches\n",
    "    avg_ms_ssim = total_ms_ssim / num_batches\n",
    "    avg_ad = total_ad / num_batches\n",
    "    val_time = time.time() - val_start_time\n",
    "    \n",
    "    logger.info(f'Validation completed in {val_time:.2f}s - '\n",
    "                f'Avg Loss: {avg_val_loss:.4f}, Avg MS-SSIM: {avg_ms_ssim:.4f}, Avg AD: {avg_ad:.4f}')\n",
    "    if validation_errors > 0:\n",
    "        logger.warning(f'Validation errors: {validation_errors}/{len(val_loader)}')\n",
    "    \n",
    "    return avg_val_loss, avg_ms_ssim, avg_ad\n",
    "\n",
    "class TrainingMetrics:\n",
    "    \"\"\"Enhanced metrics tracking v·ªõi validation loss v√† visualization\"\"\"\n",
    "    def __init__(self, output_dir):\n",
    "        self.output_dir = output_dir\n",
    "        self.metrics = defaultdict(list)\n",
    "        self.best_metrics = {\n",
    "            'best_train_loss': float('inf'),\n",
    "            'best_train_loss_epoch': 0,\n",
    "            'best_val_loss': float('inf'),\n",
    "            'best_val_loss_epoch': 0,\n",
    "            'best_ms_ssim': 0.0,\n",
    "            'best_ms_ssim_epoch': 0,\n",
    "            'best_ad': float('inf'),\n",
    "            'best_ad_epoch': 0,\n",
    "        }\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def update(self, epoch, **kwargs):\n",
    "        \"\"\"Update metrics v·ªõi validation\"\"\"\n",
    "        self.metrics['epoch'].append(epoch)\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and np.isfinite(value):\n",
    "                self.metrics[key].append(value)\n",
    "            \n",
    "        # Update best metrics\n",
    "        if 'train_loss' in kwargs and kwargs['train_loss'] is not None:\n",
    "            if kwargs['train_loss'] < self.best_metrics['best_train_loss']:\n",
    "                self.best_metrics['best_train_loss'] = kwargs['train_loss']\n",
    "                self.best_metrics['best_train_loss_epoch'] = epoch\n",
    "                \n",
    "        if 'val_loss' in kwargs and kwargs['val_loss'] is not None:\n",
    "            if kwargs['val_loss'] < self.best_metrics['best_val_loss']:\n",
    "                self.best_metrics['best_val_loss'] = kwargs['val_loss']\n",
    "                self.best_metrics['best_val_loss_epoch'] = epoch\n",
    "                \n",
    "        if 'val_ms_ssim' in kwargs and kwargs['val_ms_ssim'] is not None:\n",
    "            if kwargs['val_ms_ssim'] > self.best_metrics['best_ms_ssim']:\n",
    "                self.best_metrics['best_ms_ssim'] = kwargs['val_ms_ssim']\n",
    "                self.best_metrics['best_ms_ssim_epoch'] = epoch\n",
    "                \n",
    "        if 'val_ad' in kwargs and kwargs['val_ad'] is not None:\n",
    "            if kwargs['val_ad'] < self.best_metrics['best_ad']:\n",
    "                self.best_metrics['best_ad'] = kwargs['val_ad']\n",
    "                self.best_metrics['best_ad_epoch'] = epoch\n",
    "    \n",
    "    def save_metrics(self):\n",
    "        \"\"\"Save metrics v·ªõi timestamp\"\"\"\n",
    "        metrics_file = os.path.join(self.output_dir, 'training_metrics.json')\n",
    "        all_metrics = {\n",
    "            'training_history': dict(self.metrics),\n",
    "            'best_metrics': self.best_metrics,\n",
    "            'total_training_time': time.time() - self.start_time,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        with open(metrics_file, 'w') as f:\n",
    "            json.dump(all_metrics, f, indent=2)\n",
    "    \n",
    "    def plot_training_curves(self):\n",
    "        \"\"\"T·∫°o training curves v·ªõi validation loss\"\"\"\n",
    "        try:\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "            \n",
    "            # Training loss\n",
    "            if 'train_loss' in self.metrics:\n",
    "                axes[0,0].plot(self.metrics['epoch'], self.metrics['train_loss'], label='Train Loss', color='blue')\n",
    "                axes[0,0].set_title('Training Loss')\n",
    "                axes[0,0].set_xlabel('Epoch')\n",
    "                axes[0,0].set_ylabel('Loss')\n",
    "                axes[0,0].grid(True)\n",
    "                axes[0,0].legend()\n",
    "            \n",
    "            # Validation loss\n",
    "            if 'val_loss' in self.metrics:\n",
    "                epochs = [self.metrics['epoch'][i] for i in range(len(self.metrics['val_loss'])) \n",
    "                         if self.metrics['val_loss'][i] is not None]\n",
    "                values = [v for v in self.metrics['val_loss'] if v is not None]\n",
    "                if values:\n",
    "                    axes[0,1].plot(epochs, values, label='Val Loss', color='red')\n",
    "                    axes[0,1].set_title('Validation Loss')\n",
    "                    axes[0,1].set_xlabel('Epoch')\n",
    "                    axes[0,1].set_ylabel('Loss')\n",
    "                    axes[0,1].grid(True)\n",
    "                    axes[0,1].legend()\n",
    "            \n",
    "            # Combined losses\n",
    "            if 'train_loss' in self.metrics and 'val_loss' in self.metrics:\n",
    "                axes[0,2].plot(self.metrics['epoch'], self.metrics['train_loss'], label='Train Loss', color='blue')\n",
    "                val_epochs = [self.metrics['epoch'][i] for i in range(len(self.metrics['val_loss'])) \n",
    "                             if self.metrics['val_loss'][i] is not None]\n",
    "                val_values = [v for v in self.metrics['val_loss'] if v is not None]\n",
    "                if val_values:\n",
    "                    axes[0,2].plot(val_epochs, val_values, label='Val Loss', color='red')\n",
    "                axes[0,2].set_title('Training vs Validation Loss')\n",
    "                axes[0,2].set_xlabel('Epoch')\n",
    "                axes[0,2].set_ylabel('Loss')\n",
    "                axes[0,2].grid(True)\n",
    "                axes[0,2].legend()\n",
    "            \n",
    "            # Learning rate\n",
    "            if 'learning_rate' in self.metrics:\n",
    "                axes[1,0].plot(self.metrics['epoch'], self.metrics['learning_rate'])\n",
    "                axes[1,0].set_title('Learning Rate')\n",
    "                axes[1,0].set_xlabel('Epoch')\n",
    "                axes[1,0].set_ylabel('LR')\n",
    "                axes[1,0].set_yscale('log')\n",
    "                axes[1,0].grid(True)\n",
    "            \n",
    "            # Validation MS-SSIM\n",
    "            if 'val_ms_ssim' in self.metrics:\n",
    "                epochs = [self.metrics['epoch'][i] for i in range(len(self.metrics['val_ms_ssim'])) \n",
    "                         if self.metrics['val_ms_ssim'][i] is not None]\n",
    "                values = [v for v in self.metrics['val_ms_ssim'] if v is not None]\n",
    "                if values:\n",
    "                    axes[1,1].plot(epochs, values, color='green')\n",
    "                    axes[1,1].set_title('Validation MS-SSIM')\n",
    "                    axes[1,1].set_xlabel('Epoch')\n",
    "                    axes[1,1].set_ylabel('MS-SSIM')\n",
    "                    axes[1,1].grid(True)\n",
    "            \n",
    "            # Validation AD\n",
    "            if 'val_ad' in self.metrics:\n",
    "                epochs = [self.metrics['epoch'][i] for i in range(len(self.metrics['val_ad'])) \n",
    "                         if self.metrics['val_ad'][i] is not None]\n",
    "                values = [v for v in self.metrics['val_ad'] if v is not None]\n",
    "                if values:\n",
    "                    axes[1,2].plot(epochs, values, color='orange')\n",
    "                    axes[1,2].set_title('Validation AD')\n",
    "                    axes[1,2].set_xlabel('Epoch')\n",
    "                    axes[1,2].set_ylabel('AD')\n",
    "                    axes[1,2].grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plot_path = os.path.join(self.output_dir, 'training_curves.png')\n",
    "            plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating plots: {e}\")\n",
    "    \n",
    "    def get_best_info(self):\n",
    "        \"\"\"Get formatted best metrics info v·ªõi validation loss\"\"\"\n",
    "        return (f\"Best Train Loss: {self.best_metrics['best_train_loss']:.4f} (epoch {self.best_metrics['best_train_loss_epoch']}), \"\n",
    "                f\"Best Val Loss: {self.best_metrics['best_val_loss']:.4f} (epoch {self.best_metrics['best_val_loss_epoch']}), \"\n",
    "                f\"Best MS-SSIM: {self.best_metrics['best_ms_ssim']:.4f} (epoch {self.best_metrics['best_ms_ssim_epoch']}), \"\n",
    "                f\"Best AD: {self.best_metrics['best_ad']:.4f} (epoch {self.best_metrics['best_ad_epoch']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a49e6-5efb-45d6-abd9-3ab31e2b31c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 10:17:37 \u001b[32mINFO     \u001b[0m 492157266.py: Logger initialized successfully\u001b[0m\n",
      "2025-09-30 10:17:37 \u001b[32mINFO     \u001b[0m 2906090605.py: === SYSTEM INFORMATION ===\u001b[0m\n",
      "2025-09-30 10:17:37 \u001b[32mINFO     \u001b[0m 2906090605.py: PyTorch version: 2.8.0+cu129\u001b[0m\n",
      "2025-09-30 10:17:37 \u001b[32mINFO     \u001b[0m 2906090605.py: CUDA available: True\u001b[0m\n",
      "2025-09-30 10:17:37 \u001b[32mINFO     \u001b[0m 2906090605.py: CUDA version: 12.9\u001b[0m\n",
      "2025-09-30 10:17:37 \u001b[32mINFO     \u001b[0m 2906090605.py: GPU name: NVIDIA GeForce RTX 5090\u001b[0m\n",
      "2025-09-30 10:17:37 \u001b[32mINFO     \u001b[0m 2906090605.py: GPU memory: 33.7 GB\u001b[0m\n",
      "2025-09-30 10:17:37 \u001b[32mINFO     \u001b[0m 2906090605.py: === TRAINING CONFIGURATION ===\u001b[0m\n",
      "2025-09-30 10:17:37 \u001b[32mINFO     \u001b[0m 2906090605.py: Epochs: 100\u001b[0m\n",
      "2025-09-30 10:17:37 \u001b[32mINFO     \u001b[0m 2906090605.py: Batch size: 8\u001b[0m\n",
      "2025-09-30 10:17:37 \u001b[32mINFO     \u001b[0m 2906090605.py: Accumulation steps: 4\u001b[0m\n",
      "2025-09-30 10:17:37 \u001b[32mINFO     \u001b[0m 2906090605.py: Learning rate: 0.001\u001b[0m\n",
      "2025-09-30 10:17:37 \u001b[32mINFO     \u001b[0m 2906090605.py: GPU ID: 0\u001b[0m\n",
      "2025-09-30 10:17:37 \u001b[32mINFO     \u001b[0m 2906090605.py: Mixed Precision: True\u001b[0m\n",
      "2025-09-30 10:17:37 \u001b[32mINFO     \u001b[0m 2906090605.py: Gradient Clipping: 1.0\u001b[0m\n",
      "2025-09-30 10:17:37 \u001b[32mINFO     \u001b[0m 2906090605.py: Eval interval: 5\u001b[0m\n",
      "2025-09-30 10:17:37 \u001b[32mINFO     \u001b[0m 2906090605.py: ==============================\u001b[0m\n",
      "2025-09-30 10:17:37 \u001b[32mINFO     \u001b[0m 2906090605.py: Training with GPU 0 and PyTorch 2.8.0+cu129\u001b[0m\n",
      "2025-09-30 10:17:37 \u001b[32mINFO     \u001b[0m 2906090605.py: Training samples: 4800\u001b[0m\n",
      "2025-09-30 10:17:37 \u001b[32mINFO     \u001b[0m 2906090605.py: Validation samples: 1200\u001b[0m\n",
      "2025-09-30 10:17:38 \u001b[32mINFO     \u001b[0m 2906090605.py: Model parameters: 962,844 (trainable: 962,844)\u001b[0m\n",
      "2025-09-30 10:17:38 \u001b[32mINFO     \u001b[0m 2906090605.py: Using Automatic Mixed Precision (AMP)\u001b[0m\n",
      "2025-09-30 10:17:38 \u001b[32mINFO     \u001b[0m 2906090605.py: Using gradient clipping with max norm: 1.0\u001b[0m\n",
      "2025-09-30 10:17:38 \u001b[32mINFO     \u001b[0m 2906090605.py: Effective batch size: 32\u001b[0m\n",
      "2025-09-30 10:17:38 \u001b[32mINFO     \u001b[0m 2906090605.py: üöÄ Starting training...\u001b[0m\n",
      "NaN or Inf found in input tensor.\n",
      "2025-09-30 10:19:06 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [  10/600] Loss: 85625.2734 (avg: 84786.2969) Time: 0.38s (avg: 8.54s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:19:10 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [  20/600] Loss: 85164.9297 (avg: 85071.7402) Time: 0.39s (avg: 4.46s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:19:14 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [  30/600] Loss: 84871.3047 (avg: 85104.5188) Time: 0.38s (avg: 3.10s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:19:17 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [  40/600] Loss: 83329.8359 (avg: 84930.9473) Time: 0.39s (avg: 2.42s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:19:21 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [  50/600] Loss: 84821.7031 (avg: 84965.3659) Time: 0.38s (avg: 2.02s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:19:25 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [  60/600] Loss: 85575.9375 (avg: 84994.9969) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:19:29 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [  70/600] Loss: 85349.1875 (avg: 84829.4223) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:19:33 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [  80/600] Loss: 84977.2266 (avg: 84841.5586) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:19:37 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [  90/600] Loss: 85280.3594 (avg: 84893.0008) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:19:41 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 100/600] Loss: 85085.3828 (avg: 84853.7478) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:19:44 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 110/600] Loss: 85221.3672 (avg: 84769.1098) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:19:48 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 120/600] Loss: 84364.6875 (avg: 84745.8528) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:19:52 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 130/600] Loss: 84576.8125 (avg: 84591.1467) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:19:56 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 140/600] Loss: 83814.9844 (avg: 84513.1559) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:20:00 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 150/600] Loss: 85217.2422 (avg: 84430.1014) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:20:04 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 160/600] Loss: 84454.1016 (avg: 84414.4969) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:20:07 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 170/600] Loss: 84433.0312 (avg: 84464.7516) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:20:11 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 180/600] Loss: 84532.9375 (avg: 84427.8669) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:20:15 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 190/600] Loss: 82577.5703 (avg: 84342.8475) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:20:19 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 200/600] Loss: 83947.7031 (avg: 84244.8192) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:20:23 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 210/600] Loss: 82965.0547 (avg: 84167.3364) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:20:27 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 220/600] Loss: 84199.2188 (avg: 84054.7050) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:20:31 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 230/600] Loss: 83229.0312 (avg: 83938.3270) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:20:34 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 240/600] Loss: 84255.4688 (avg: 83918.4680) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:20:38 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 250/600] Loss: 83966.2031 (avg: 83870.8920) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:20:42 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 260/600] Loss: 84352.9297 (avg: 83815.7619) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:20:46 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 270/600] Loss: 83093.4219 (avg: 83760.9142) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:20:50 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 280/600] Loss: 84500.3672 (avg: 83726.8181) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:20:54 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 290/600] Loss: 82368.4609 (avg: 83673.2273) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:20:58 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 300/600] Loss: 82279.4844 (avg: 83573.0386) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:21:01 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 310/600] Loss: 81477.7266 (avg: 83463.5120) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:21:05 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 320/600] Loss: 82623.8359 (avg: 83354.1958) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:21:09 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 330/600] Loss: 83536.6562 (avg: 83302.4100) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:21:13 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 340/600] Loss: 83178.2344 (avg: 83245.2787) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:21:17 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 350/600] Loss: 82407.0078 (avg: 83156.9489) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:21:21 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 360/600] Loss: 82823.1953 (avg: 83180.4492) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:21:25 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 370/600] Loss: 82864.4531 (avg: 83170.0875) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:21:28 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 380/600] Loss: 83316.7188 (avg: 83070.9091) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:21:32 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 390/600] Loss: 81806.6094 (avg: 82964.5433) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:21:36 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 400/600] Loss: 83166.3047 (avg: 83000.4878) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:21:40 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 410/600] Loss: 82672.7656 (avg: 82870.2986) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:21:44 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 420/600] Loss: 81482.3125 (avg: 82751.5791) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:21:48 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 430/600] Loss: 82511.9219 (avg: 82754.2173) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:21:52 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 440/600] Loss: 81509.4766 (avg: 82711.2156) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:21:55 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 450/600] Loss: 82849.6797 (avg: 82591.3880) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:21:59 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 460/600] Loss: 83264.6562 (avg: 82575.5434) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:22:03 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 470/600] Loss: 82015.4375 (avg: 82590.0511) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:22:07 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 480/600] Loss: 82332.5312 (avg: 82524.3820) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:22:11 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 490/600] Loss: 82008.9766 (avg: 82481.8361) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:22:15 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 500/600] Loss: 81632.0312 (avg: 82338.9931) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:22:19 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 510/600] Loss: 81921.6328 (avg: 82242.8469) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:22:22 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 520/600] Loss: 81974.3984 (avg: 82123.3269) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:22:26 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 530/600] Loss: 82250.1406 (avg: 82044.3306) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:22:30 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 540/600] Loss: 81423.4375 (avg: 81918.4200) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:22:34 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 550/600] Loss: 81280.4375 (avg: 81906.2342) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:22:38 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 560/600] Loss: 81247.4844 (avg: 81772.5192) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:22:42 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 570/600] Loss: 82061.2656 (avg: 81658.3619) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:22:46 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 580/600] Loss: 82264.9688 (avg: 81563.3491) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:22:49 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 590/600] Loss: 80436.4922 (avg: 81507.4214) Time: 0.38s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:22:53 \u001b[32mINFO     \u001b[0m 2906090605.py: [  1/100] [ 600/600] Loss: 80725.2500 (avg: 81463.8911) Time: 0.39s (avg: 0.38s) LR: 1.00e-03\u001b[0m\n",
      "2025-09-30 10:22:53 \u001b[32mINFO     \u001b[0m 2906090605.py: === EPOCH   1/100 SUMMARY ===\u001b[0m\n",
      "2025-09-30 10:22:53 \u001b[32mINFO     \u001b[0m 2906090605.py: Train loss: 83366.325677\u001b[0m\n",
      "2025-09-30 10:22:53 \u001b[32mINFO     \u001b[0m 2906090605.py: Epoch time: 315.26s\u001b[0m\n",
      "2025-09-30 10:22:53 \u001b[32mINFO     \u001b[0m 2906090605.py: Avg batch time: 0.520s\u001b[0m\n",
      "2025-09-30 10:22:53 \u001b[32mINFO     \u001b[0m 2906090605.py: Throughput: 15.2 samples/sec\u001b[0m\n",
      "2025-09-30 10:22:53 \u001b[32mINFO     \u001b[0m 2906090605.py: Learning rate: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:22:53 \u001b[32mINFO     \u001b[0m 2906090605.py: üìä Best Train Loss: 83366.3257 (epoch 1), Best Val Loss: inf (epoch 0), Best MS-SSIM: 0.0000 (epoch 0), Best AD: inf (epoch 0)\u001b[0m\n",
      "2025-09-30 10:22:53 \u001b[32mINFO     \u001b[0m 2906090605.py: ============================================================\u001b[0m\n",
      "2025-09-30 10:22:58 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [  10/600] Loss: 80650.3828 (avg: 80975.5312) Time: 0.38s (avg: 0.39s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:23:02 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [  20/600] Loss: 80822.8281 (avg: 80978.0898) Time: 0.39s (avg: 0.39s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:23:06 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [  30/600] Loss: 81751.9219 (avg: 80997.3661) Time: 0.38s (avg: 0.39s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:23:10 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [  40/600] Loss: 81215.3984 (avg: 80973.3678) Time: 0.39s (avg: 0.39s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:23:14 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [  50/600] Loss: 80713.1719 (avg: 80996.9381) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:23:18 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [  60/600] Loss: 81521.3281 (avg: 80955.8836) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:23:21 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [  70/600] Loss: 80144.5000 (avg: 80854.6906) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:23:25 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [  80/600] Loss: 80588.0703 (avg: 80761.2242) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:23:29 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [  90/600] Loss: 81597.5391 (avg: 80774.5306) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:23:33 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 100/600] Loss: 81023.4141 (avg: 80748.7038) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:23:37 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 110/600] Loss: 79846.5078 (avg: 80645.1800) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:23:41 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 120/600] Loss: 80325.1484 (avg: 80701.0422) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:23:45 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 130/600] Loss: 81195.2188 (avg: 80653.7009) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:23:48 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 140/600] Loss: 80246.2422 (avg: 80512.7936) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:23:52 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 150/600] Loss: 79733.9766 (avg: 80357.4919) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:23:56 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 160/600] Loss: 79868.7500 (avg: 80292.1011) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:24:00 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 170/600] Loss: 80983.7656 (avg: 80152.0942) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:24:04 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 180/600] Loss: 79729.1641 (avg: 80069.0798) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:24:08 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 190/600] Loss: 79018.6094 (avg: 79987.4022) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:24:12 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 200/600] Loss: 79436.4844 (avg: 79909.3769) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:24:15 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 210/600] Loss: 79049.4609 (avg: 79831.0733) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:24:19 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 220/600] Loss: 79763.5312 (avg: 79670.1308) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:24:23 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 230/600] Loss: 80064.4297 (avg: 79574.8661) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:24:27 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 240/600] Loss: 78976.3984 (avg: 79443.6463) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:24:31 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 250/600] Loss: 79058.6250 (avg: 79278.3869) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:24:35 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 260/600] Loss: 78092.6484 (avg: 79129.4772) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:24:39 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 270/600] Loss: 78833.0781 (avg: 79114.3533) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:24:42 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 280/600] Loss: 78988.2656 (avg: 79051.4336) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:24:46 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 290/600] Loss: 79388.6797 (avg: 78979.9203) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:24:50 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 300/600] Loss: 79773.6484 (avg: 78950.6180) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:24:54 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 310/600] Loss: 78039.8125 (avg: 78873.9102) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:24:58 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 320/600] Loss: 79063.1172 (avg: 78708.4694) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:25:02 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 330/600] Loss: 78655.3125 (avg: 78569.2787) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:25:06 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 340/600] Loss: 78172.7734 (avg: 78434.3731) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:25:09 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 350/600] Loss: 78273.5625 (avg: 78225.2197) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:25:13 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 360/600] Loss: 78128.6484 (avg: 78112.5323) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:25:17 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 370/600] Loss: 78221.5938 (avg: 78081.6336) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:25:21 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 380/600] Loss: 76997.4688 (avg: 77961.1636) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:25:25 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 390/600] Loss: 77878.5156 (avg: 77921.8442) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:25:29 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 400/600] Loss: 78213.2422 (avg: 77901.1122) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:25:33 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 410/600] Loss: 76567.4844 (avg: 77880.4367) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:25:36 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 420/600] Loss: 77338.2266 (avg: 77703.2328) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:25:40 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 430/600] Loss: 77029.0859 (avg: 77597.5211) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:25:44 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 440/600] Loss: 76533.4688 (avg: 77509.4155) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:25:48 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 450/600] Loss: 76949.4531 (avg: 77353.4414) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:25:52 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 460/600] Loss: 77284.0391 (avg: 77209.1555) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:25:56 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 470/600] Loss: 77009.5625 (avg: 77107.3452) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:26:00 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 480/600] Loss: 77568.3594 (avg: 77016.4816) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:26:03 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 490/600] Loss: 76874.1875 (avg: 76788.3414) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:26:07 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 500/600] Loss: 78410.9062 (avg: 76776.8756) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:26:11 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 510/600] Loss: 77138.1953 (avg: 76631.1798) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:26:15 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 520/600] Loss: 76663.7422 (avg: 76538.8533) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:26:19 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 530/600] Loss: 76427.3984 (avg: 76473.7389) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:26:23 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 540/600] Loss: 75713.7188 (avg: 76414.1367) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:26:27 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 550/600] Loss: 75960.9141 (avg: 76286.1234) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:26:30 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 560/600] Loss: 75993.0234 (avg: 76215.0241) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:26:34 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 570/600] Loss: 74863.5078 (avg: 76114.8814) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:26:38 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 580/600] Loss: 76333.8047 (avg: 76006.0048) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:26:42 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 590/600] Loss: 74276.4141 (avg: 75923.1278) Time: 0.38s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:26:46 \u001b[32mINFO     \u001b[0m 2906090605.py: [  2/100] [ 600/600] Loss: 74566.4297 (avg: 75780.6930) Time: 0.39s (avg: 0.38s) LR: 9.76e-04\u001b[0m\n",
      "2025-09-30 10:26:46 \u001b[32mINFO     \u001b[0m 2906090605.py: === EPOCH   2/100 SUMMARY ===\u001b[0m\n",
      "2025-09-30 10:26:46 \u001b[32mINFO     \u001b[0m 2906090605.py: Train loss: 78547.081732\u001b[0m\n",
      "2025-09-30 10:26:46 \u001b[32mINFO     \u001b[0m 2906090605.py: Epoch time: 232.47s\u001b[0m\n",
      "2025-09-30 10:26:46 \u001b[32mINFO     \u001b[0m 2906090605.py: Avg batch time: 0.384s\u001b[0m\n",
      "2025-09-30 10:26:46 \u001b[32mINFO     \u001b[0m 2906090605.py: Throughput: 20.6 samples/sec\u001b[0m\n",
      "2025-09-30 10:26:46 \u001b[32mINFO     \u001b[0m 2906090605.py: Learning rate: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:26:46 \u001b[32mINFO     \u001b[0m 2906090605.py: üìä Best Train Loss: 78547.0817 (epoch 2), Best Val Loss: inf (epoch 0), Best MS-SSIM: 0.0000 (epoch 0), Best AD: inf (epoch 0)\u001b[0m\n",
      "2025-09-30 10:26:46 \u001b[32mINFO     \u001b[0m 2906090605.py: ============================================================\u001b[0m\n",
      "2025-09-30 10:26:51 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [  10/600] Loss: 75454.8438 (avg: 75417.4516) Time: 0.38s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:26:55 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [  20/600] Loss: 73919.2422 (avg: 75211.8895) Time: 0.39s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:26:59 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [  30/600] Loss: 75156.8438 (avg: 75177.8721) Time: 0.38s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:27:03 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [  40/600] Loss: 75180.9688 (avg: 75173.5482) Time: 0.39s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:27:07 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [  50/600] Loss: 75229.9375 (avg: 75138.0627) Time: 0.38s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:27:10 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [  60/600] Loss: 74427.0625 (avg: 74998.4580) Time: 0.39s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:27:14 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [  70/600] Loss: 73639.9766 (avg: 74902.5641) Time: 0.39s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:27:18 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [  80/600] Loss: 75717.2422 (avg: 74825.9372) Time: 0.39s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:27:22 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [  90/600] Loss: 74478.9531 (avg: 74751.7703) Time: 0.38s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:27:26 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 100/600] Loss: 74958.3203 (avg: 74635.3692) Time: 0.39s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:27:30 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 110/600] Loss: 73951.9531 (avg: 74489.5144) Time: 0.38s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:27:34 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 120/600] Loss: 74686.4531 (avg: 74440.1252) Time: 0.39s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:27:37 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 130/600] Loss: 73080.9219 (avg: 74290.9781) Time: 0.38s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:27:41 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 140/600] Loss: 72962.4922 (avg: 74010.2883) Time: 0.39s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:27:45 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 150/600] Loss: 74110.3281 (avg: 73918.7870) Time: 0.38s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:27:49 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 160/600] Loss: 73567.6562 (avg: 73854.5109) Time: 0.39s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:27:53 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 170/600] Loss: 74028.1797 (avg: 73758.2223) Time: 0.38s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:27:57 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 180/600] Loss: 73120.2734 (avg: 73621.8147) Time: 0.39s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:28:01 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 190/600] Loss: 73594.8750 (avg: 73587.1987) Time: 0.38s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:28:04 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 200/600] Loss: 72868.4062 (avg: 73440.6205) Time: 0.39s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:28:08 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 210/600] Loss: 72667.5625 (avg: 73300.7222) Time: 0.38s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:28:12 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 220/600] Loss: 71878.3438 (avg: 73087.5944) Time: 0.40s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:28:16 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 230/600] Loss: 73096.9141 (avg: 72956.9364) Time: 0.38s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:28:20 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 240/600] Loss: 71741.5938 (avg: 72825.1167) Time: 0.39s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:28:24 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 250/600] Loss: 71797.4609 (avg: 72661.5156) Time: 0.38s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:28:28 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 260/600] Loss: 72610.1406 (avg: 72562.3488) Time: 0.39s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:28:31 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 270/600] Loss: 72083.3203 (avg: 72504.8459) Time: 0.38s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:28:35 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 280/600] Loss: 72348.4062 (avg: 72420.9130) Time: 0.39s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:28:39 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 290/600] Loss: 71669.7188 (avg: 72345.9317) Time: 0.38s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:28:43 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 300/600] Loss: 71338.3828 (avg: 72270.4956) Time: 0.39s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:28:47 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 310/600] Loss: 70314.8203 (avg: 72080.5708) Time: 0.38s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:28:51 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 320/600] Loss: 72055.8594 (avg: 71990.1431) Time: 0.39s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:28:55 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 330/600] Loss: 71225.0391 (avg: 71826.5737) Time: 0.38s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:28:59 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 340/600] Loss: 72362.0000 (avg: 71714.1833) Time: 0.39s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:29:02 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 350/600] Loss: 71733.2656 (avg: 71591.0894) Time: 0.38s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:29:06 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 360/600] Loss: 70987.6172 (avg: 71529.9581) Time: 0.39s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:29:10 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 370/600] Loss: 70580.5938 (avg: 71355.9302) Time: 0.38s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:29:14 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 380/600] Loss: 70101.0000 (avg: 71275.7319) Time: 0.39s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:29:18 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 390/600] Loss: 71454.3438 (avg: 71119.7528) Time: 0.38s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:29:22 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 400/600] Loss: 70624.9531 (avg: 70977.8295) Time: 0.39s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:29:26 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 410/600] Loss: 69862.7891 (avg: 70766.1553) Time: 0.38s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:29:29 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 420/600] Loss: 69959.2109 (avg: 70643.7173) Time: 0.39s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:29:33 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 430/600] Loss: 70717.0859 (avg: 70500.2214) Time: 0.38s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:29:37 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 440/600] Loss: 69983.7578 (avg: 70327.2364) Time: 0.39s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:29:41 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 450/600] Loss: 70982.6250 (avg: 70236.3086) Time: 0.38s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:29:45 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 460/600] Loss: 70413.9062 (avg: 70166.4716) Time: 0.39s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:29:49 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 470/600] Loss: 69215.5781 (avg: 69977.3347) Time: 0.38s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:29:53 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 480/600] Loss: 69751.5625 (avg: 69865.3078) Time: 0.39s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:29:56 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 490/600] Loss: 69319.9219 (avg: 69784.1163) Time: 0.38s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:30:00 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 500/600] Loss: 69583.8438 (avg: 69695.4523) Time: 0.39s (avg: 0.39s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:30:04 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 510/600] Loss: 69804.8359 (avg: 69557.4366) Time: 0.38s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:30:08 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 520/600] Loss: 69230.6719 (avg: 69489.5755) Time: 0.39s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:30:12 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 530/600] Loss: 68850.5625 (avg: 69315.6728) Time: 0.38s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:30:16 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 540/600] Loss: 68005.8203 (avg: 69163.9781) Time: 0.39s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:30:20 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 550/600] Loss: 69129.4297 (avg: 68978.5712) Time: 0.38s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:30:23 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 560/600] Loss: 67519.2266 (avg: 68832.2156) Time: 0.39s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:30:27 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 570/600] Loss: 68467.7422 (avg: 68683.8883) Time: 0.38s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:30:31 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 580/600] Loss: 69465.7578 (avg: 68605.8566) Time: 0.39s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:30:35 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 590/600] Loss: 67728.7266 (avg: 68493.6925) Time: 0.38s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:30:39 \u001b[32mINFO     \u001b[0m 2906090605.py: [  3/100] [ 600/600] Loss: 68624.8281 (avg: 68446.3359) Time: 0.39s (avg: 0.38s) LR: 9.05e-04\u001b[0m\n",
      "2025-09-30 10:30:39 \u001b[32mINFO     \u001b[0m 2906090605.py: === EPOCH   3/100 SUMMARY ===\u001b[0m\n",
      "2025-09-30 10:30:39 \u001b[32mINFO     \u001b[0m 2906090605.py: Train loss: 71832.536471\u001b[0m\n",
      "2025-09-30 10:30:39 \u001b[32mINFO     \u001b[0m 2906090605.py: Epoch time: 232.99s\u001b[0m\n",
      "2025-09-30 10:30:39 \u001b[32mINFO     \u001b[0m 2906090605.py: Avg batch time: 0.385s\u001b[0m\n",
      "2025-09-30 10:30:39 \u001b[32mINFO     \u001b[0m 2906090605.py: Throughput: 20.6 samples/sec\u001b[0m\n",
      "2025-09-30 10:30:39 \u001b[32mINFO     \u001b[0m 2906090605.py: Learning rate: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:30:39 \u001b[32mINFO     \u001b[0m 2906090605.py: üìä Best Train Loss: 71832.5365 (epoch 3), Best Val Loss: inf (epoch 0), Best MS-SSIM: 0.0000 (epoch 0), Best AD: inf (epoch 0)\u001b[0m\n",
      "2025-09-30 10:30:39 \u001b[32mINFO     \u001b[0m 2906090605.py: ============================================================\u001b[0m\n",
      "2025-09-30 10:30:44 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [  10/600] Loss: 68480.3828 (avg: 67700.9773) Time: 0.38s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:30:48 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [  20/600] Loss: 68032.5312 (avg: 67821.9633) Time: 0.39s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:30:52 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [  30/600] Loss: 67813.3438 (avg: 67916.6477) Time: 0.38s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:30:55 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [  40/600] Loss: 67584.3516 (avg: 67927.6311) Time: 0.39s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:30:59 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [  50/600] Loss: 67394.7656 (avg: 67866.0742) Time: 0.38s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:31:03 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [  60/600] Loss: 67322.1484 (avg: 67815.9958) Time: 0.39s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:31:07 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [  70/600] Loss: 66702.5312 (avg: 67644.2948) Time: 0.38s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:31:11 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [  80/600] Loss: 67886.2109 (avg: 67471.7269) Time: 0.39s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:31:15 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [  90/600] Loss: 66817.9219 (avg: 67276.5069) Time: 0.38s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:31:19 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 100/600] Loss: 67104.6406 (avg: 67169.0750) Time: 0.39s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:31:22 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 110/600] Loss: 67707.1641 (avg: 67085.5328) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:31:26 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 120/600] Loss: 66737.9844 (avg: 67031.7116) Time: 0.39s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:31:30 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 130/600] Loss: 66681.5781 (avg: 66858.4855) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:31:34 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 140/600] Loss: 65932.4375 (avg: 66698.9932) Time: 0.39s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:31:38 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 150/600] Loss: 66091.4922 (avg: 66520.4057) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:31:42 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 160/600] Loss: 66183.3828 (avg: 66330.5185) Time: 0.39s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:31:46 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 170/600] Loss: 67242.7969 (avg: 66205.6295) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:31:49 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 180/600] Loss: 65667.1016 (avg: 66132.4312) Time: 0.39s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:31:53 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 190/600] Loss: 65653.5391 (avg: 66055.7645) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:31:57 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 200/600] Loss: 65041.9883 (avg: 65961.9962) Time: 0.39s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:32:01 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 210/600] Loss: 65486.7656 (avg: 65880.1251) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:32:05 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 220/600] Loss: 64883.7500 (avg: 65758.9792) Time: 0.39s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:32:09 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 230/600] Loss: 64708.9180 (avg: 65622.7158) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:32:13 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 240/600] Loss: 65323.0586 (avg: 65517.2074) Time: 0.39s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:32:16 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 250/600] Loss: 65283.0156 (avg: 65384.3877) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:32:20 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 260/600] Loss: 64880.7461 (avg: 65239.9849) Time: 0.39s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:32:24 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 270/600] Loss: 64963.0781 (avg: 65079.4784) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:32:28 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 280/600] Loss: 65030.8203 (avg: 65031.5982) Time: 0.39s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:32:32 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 290/600] Loss: 64946.4844 (avg: 64980.5744) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:32:36 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 300/600] Loss: 64700.4531 (avg: 64859.9641) Time: 0.39s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:32:40 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 310/600] Loss: 64274.9414 (avg: 64753.1411) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:32:44 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 320/600] Loss: 63446.8477 (avg: 64587.4184) Time: 0.39s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:32:47 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 330/600] Loss: 64117.1367 (avg: 64416.5400) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:32:51 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 340/600] Loss: 63553.0273 (avg: 64236.5977) Time: 0.39s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:32:55 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 350/600] Loss: 63826.0156 (avg: 64106.3484) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:32:59 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 360/600] Loss: 63467.7617 (avg: 64009.2679) Time: 0.39s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:33:03 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 370/600] Loss: 63460.2266 (avg: 63898.0869) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:33:07 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 380/600] Loss: 63801.4766 (avg: 63733.3144) Time: 0.39s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:33:11 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 390/600] Loss: 64374.3047 (avg: 63597.7840) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:33:14 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 400/600] Loss: 63512.3242 (avg: 63441.2970) Time: 0.39s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:33:18 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 410/600] Loss: 62660.7188 (avg: 63249.6877) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:33:22 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 420/600] Loss: 63936.7695 (avg: 63164.6059) Time: 0.39s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:33:26 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 430/600] Loss: 62794.9531 (avg: 63053.2665) Time: 0.39s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:33:30 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 440/600] Loss: 62216.8555 (avg: 62911.8288) Time: 0.39s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:33:34 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 450/600] Loss: 62408.5586 (avg: 62767.8444) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:33:38 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 460/600] Loss: 61965.3086 (avg: 62626.6905) Time: 0.39s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:33:41 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 470/600] Loss: 61848.9141 (avg: 62437.0602) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:33:45 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 480/600] Loss: 62925.5117 (avg: 62372.3599) Time: 0.39s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:33:49 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 490/600] Loss: 61919.8516 (avg: 62197.9137) Time: 0.39s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:33:53 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 500/600] Loss: 61243.7383 (avg: 62093.5350) Time: 0.39s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:33:57 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 510/600] Loss: 62292.2930 (avg: 62055.7434) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:34:01 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 520/600] Loss: 61988.8633 (avg: 62056.5639) Time: 0.39s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:34:05 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 530/600] Loss: 61367.0273 (avg: 61890.2372) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:34:08 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 540/600] Loss: 61781.2344 (avg: 61786.7299) Time: 0.39s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:34:12 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 550/600] Loss: 60962.4570 (avg: 61717.2341) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:34:16 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 560/600] Loss: 60910.4414 (avg: 61514.5071) Time: 0.39s (avg: 0.39s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:34:20 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 570/600] Loss: 61064.1680 (avg: 61291.2815) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:34:24 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 580/600] Loss: 61413.5781 (avg: 61151.8737) Time: 0.39s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:34:28 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 590/600] Loss: 61235.8164 (avg: 61070.4492) Time: 0.38s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:34:32 \u001b[32mINFO     \u001b[0m 2906090605.py: [  4/100] [ 600/600] Loss: 60205.5469 (avg: 60906.9875) Time: 0.39s (avg: 0.38s) LR: 7.94e-04\u001b[0m\n",
      "2025-09-30 10:34:32 \u001b[32mINFO     \u001b[0m 2906090605.py: === EPOCH   4/100 SUMMARY ===\u001b[0m\n",
      "2025-09-30 10:34:32 \u001b[32mINFO     \u001b[0m 2906090605.py: Train loss: 64399.595762\u001b[0m\n",
      "2025-09-30 10:34:32 \u001b[32mINFO     \u001b[0m 2906090605.py: Epoch time: 232.62s\u001b[0m\n",
      "2025-09-30 10:34:32 \u001b[32mINFO     \u001b[0m 2906090605.py: Avg batch time: 0.385s\u001b[0m\n",
      "2025-09-30 10:34:32 \u001b[32mINFO     \u001b[0m 2906090605.py: Throughput: 20.6 samples/sec\u001b[0m\n",
      "2025-09-30 10:34:32 \u001b[32mINFO     \u001b[0m 2906090605.py: Learning rate: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:34:32 \u001b[32mINFO     \u001b[0m 2906090605.py: üìä Best Train Loss: 64399.5958 (epoch 4), Best Val Loss: inf (epoch 0), Best MS-SSIM: 0.0000 (epoch 0), Best AD: inf (epoch 0)\u001b[0m\n",
      "2025-09-30 10:34:32 \u001b[32mINFO     \u001b[0m 2906090605.py: ============================================================\u001b[0m\n",
      "2025-09-30 10:34:37 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [  10/600] Loss: 59520.9453 (avg: 60718.1797) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:34:41 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [  20/600] Loss: 60456.0000 (avg: 60638.4965) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:34:45 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [  30/600] Loss: 60369.5547 (avg: 60548.2732) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:34:49 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [  40/600] Loss: 60833.4062 (avg: 60512.9546) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:34:53 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [  50/600] Loss: 59957.9961 (avg: 60414.6028) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:34:56 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [  60/600] Loss: 60762.4258 (avg: 60363.0177) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:35:00 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [  70/600] Loss: 60024.7969 (avg: 60235.4262) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:35:04 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [  80/600] Loss: 60353.2305 (avg: 60126.9673) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:35:08 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [  90/600] Loss: 59370.0547 (avg: 59911.2335) Time: 0.38s (avg: 0.38s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:35:12 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 100/600] Loss: 59778.6133 (avg: 59859.8838) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:35:16 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 110/600] Loss: 58858.3945 (avg: 59657.6391) Time: 0.38s (avg: 0.38s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:35:20 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 120/600] Loss: 57836.4180 (avg: 59520.2153) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:35:24 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 130/600] Loss: 57867.4414 (avg: 59442.6537) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:35:27 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 140/600] Loss: 59227.5000 (avg: 59453.1662) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:35:33 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 150/600] Loss: 58787.3906 (avg: 59369.1905) Time: 0.38s (avg: 0.43s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:35:37 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 160/600] Loss: 58602.3320 (avg: 59318.5162) Time: 0.39s (avg: 0.43s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:35:41 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 170/600] Loss: 57430.3164 (avg: 59236.8815) Time: 0.38s (avg: 0.43s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:35:45 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 180/600] Loss: 58919.6523 (avg: 59122.7605) Time: 0.39s (avg: 0.43s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:35:49 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 190/600] Loss: 58172.6719 (avg: 59008.7755) Time: 0.38s (avg: 0.43s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:35:53 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 200/600] Loss: 58170.9258 (avg: 58831.7632) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:35:56 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 210/600] Loss: 59121.2031 (avg: 58784.6586) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:36:00 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 220/600] Loss: 57807.9570 (avg: 58723.9172) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:36:04 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 230/600] Loss: 59269.3984 (avg: 58623.7913) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:36:08 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 240/600] Loss: 59348.2617 (avg: 58573.5370) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:36:12 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 250/600] Loss: 58370.1992 (avg: 58532.3777) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:36:16 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 260/600] Loss: 57267.4297 (avg: 58249.6457) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:36:20 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 270/600] Loss: 58581.0977 (avg: 58081.3328) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:36:23 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 280/600] Loss: 58024.6836 (avg: 57926.0531) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:36:27 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 290/600] Loss: 57847.4609 (avg: 57650.9659) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:36:31 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 300/600] Loss: 57734.1875 (avg: 57509.8720) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:36:35 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 310/600] Loss: 57208.2500 (avg: 57514.5581) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:36:39 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 320/600] Loss: 58006.5469 (avg: 57425.8259) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:36:43 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 330/600] Loss: 56891.7930 (avg: 57299.3893) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:36:47 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 340/600] Loss: 57737.0703 (avg: 57277.7501) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:36:51 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 350/600] Loss: 57672.9375 (avg: 57188.4558) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:36:54 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 360/600] Loss: 57051.1289 (avg: 57044.6484) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:36:58 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 370/600] Loss: 56580.4102 (avg: 56935.9858) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:37:02 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 380/600] Loss: 56839.0508 (avg: 56877.2589) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:37:06 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 390/600] Loss: 56543.7461 (avg: 56744.8637) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:37:10 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 400/600] Loss: 56076.0898 (avg: 56527.5591) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:37:14 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 410/600] Loss: 55685.2539 (avg: 56357.3236) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:37:18 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 420/600] Loss: 57040.6719 (avg: 56229.8384) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:37:21 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 430/600] Loss: 56564.5078 (avg: 56118.3625) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:37:25 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 440/600] Loss: 56259.9531 (avg: 56042.6501) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:37:29 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 450/600] Loss: 55962.4727 (avg: 55958.7162) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:37:33 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 460/600] Loss: 55921.5898 (avg: 55894.3039) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:37:37 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 470/600] Loss: 55838.7930 (avg: 55821.3846) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:37:41 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 480/600] Loss: 55403.6680 (avg: 55660.7859) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:37:45 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 490/600] Loss: 54264.8789 (avg: 55480.5020) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:37:49 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 500/600] Loss: 54521.7656 (avg: 55433.3095) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:37:52 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 510/600] Loss: 55855.1992 (avg: 55346.4584) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:37:56 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 520/600] Loss: 54790.9688 (avg: 55229.0609) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:38:00 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 530/600] Loss: 55289.0898 (avg: 55039.5113) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:38:04 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 540/600] Loss: 55017.0195 (avg: 55023.7774) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:38:08 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 550/600] Loss: 54531.2891 (avg: 54886.2405) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:38:12 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 560/600] Loss: 53987.5820 (avg: 54754.2062) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:38:16 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 570/600] Loss: 54213.6445 (avg: 54630.7352) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:38:19 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 580/600] Loss: 54218.3789 (avg: 54656.5975) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:38:23 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 590/600] Loss: 55043.7344 (avg: 54544.9470) Time: 0.38s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:38:27 \u001b[32mINFO     \u001b[0m 2906090605.py: [  5/100] [ 600/600] Loss: 54381.4844 (avg: 54461.3952) Time: 0.39s (avg: 0.39s) LR: 6.55e-04\u001b[0m\n",
      "2025-09-30 10:38:27 \u001b[32mINFO     \u001b[0m 2906090605.py: === EPOCH   5/100 SUMMARY ===\u001b[0m\n",
      "2025-09-30 10:38:27 \u001b[32mINFO     \u001b[0m 2906090605.py: Train loss: 57414.447188\u001b[0m\n",
      "2025-09-30 10:38:27 \u001b[32mINFO     \u001b[0m 2906090605.py: Epoch time: 235.48s\u001b[0m\n",
      "2025-09-30 10:38:27 \u001b[32mINFO     \u001b[0m 2906090605.py: Avg batch time: 0.389s\u001b[0m\n",
      "2025-09-30 10:38:27 \u001b[32mINFO     \u001b[0m 2906090605.py: Throughput: 20.4 samples/sec\u001b[0m\n",
      "2025-09-30 10:38:27 \u001b[32mINFO     \u001b[0m 2906090605.py: Learning rate: 5.01e-04\u001b[0m\n",
      "2025-09-30 10:38:27 \u001b[32mINFO     \u001b[0m 2906090605.py: üîç Running validation...\u001b[0m\n",
      "2025-09-30 10:38:55 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [20/1200] - Loss: 52388.8203, MS-SSIM: 0.5820, AD: 0.8635\u001b[0m\n",
      "2025-09-30 10:38:55 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [40/1200] - Loss: 52590.4922, MS-SSIM: 0.5972, AD: 0.8631\u001b[0m\n",
      "2025-09-30 10:38:56 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [60/1200] - Loss: 55724.5898, MS-SSIM: 0.5806, AD: 0.8918\u001b[0m\n",
      "2025-09-30 10:38:56 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [80/1200] - Loss: 53620.0586, MS-SSIM: 0.5915, AD: 0.8729\u001b[0m\n",
      "2025-09-30 10:38:56 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [100/1200] - Loss: 54348.1758, MS-SSIM: 0.5803, AD: 0.8785\u001b[0m\n",
      "2025-09-30 10:38:56 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [120/1200] - Loss: 53535.1836, MS-SSIM: 0.6035, AD: 0.8700\u001b[0m\n",
      "2025-09-30 10:38:57 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [140/1200] - Loss: 54297.6602, MS-SSIM: 0.5936, AD: 0.8783\u001b[0m\n",
      "2025-09-30 10:38:57 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [160/1200] - Loss: 55485.8594, MS-SSIM: 0.5822, AD: 0.8880\u001b[0m\n",
      "2025-09-30 10:38:57 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [180/1200] - Loss: 54396.0586, MS-SSIM: 0.5864, AD: 0.8819\u001b[0m\n",
      "2025-09-30 10:38:57 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [200/1200] - Loss: 53371.2852, MS-SSIM: 0.5838, AD: 0.8710\u001b[0m\n",
      "2025-09-30 10:38:58 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [220/1200] - Loss: 54765.7500, MS-SSIM: 0.6104, AD: 0.8784\u001b[0m\n",
      "2025-09-30 10:38:58 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [240/1200] - Loss: 54524.0547, MS-SSIM: 0.5842, AD: 0.8787\u001b[0m\n",
      "2025-09-30 10:38:58 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [260/1200] - Loss: 53239.5117, MS-SSIM: 0.5852, AD: 0.8706\u001b[0m\n",
      "2025-09-30 10:38:58 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [280/1200] - Loss: 54281.7930, MS-SSIM: 0.5877, AD: 0.8810\u001b[0m\n",
      "2025-09-30 10:38:58 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [300/1200] - Loss: 55228.4453, MS-SSIM: 0.5903, AD: 0.8929\u001b[0m\n",
      "2025-09-30 10:38:59 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [320/1200] - Loss: 53079.1992, MS-SSIM: 0.5969, AD: 0.8667\u001b[0m\n",
      "2025-09-30 10:38:59 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [340/1200] - Loss: 55136.2812, MS-SSIM: 0.5788, AD: 0.8841\u001b[0m\n",
      "2025-09-30 10:38:59 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [360/1200] - Loss: 53208.9609, MS-SSIM: 0.5907, AD: 0.8699\u001b[0m\n",
      "2025-09-30 10:38:59 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [380/1200] - Loss: 53915.8594, MS-SSIM: 0.5922, AD: 0.8744\u001b[0m\n",
      "2025-09-30 10:39:00 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [400/1200] - Loss: 54847.4766, MS-SSIM: 0.5791, AD: 0.8814\u001b[0m\n",
      "2025-09-30 10:39:00 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [420/1200] - Loss: 53686.8125, MS-SSIM: 0.5890, AD: 0.8732\u001b[0m\n",
      "2025-09-30 10:39:00 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [440/1200] - Loss: 55587.0820, MS-SSIM: 0.5802, AD: 0.8890\u001b[0m\n",
      "2025-09-30 10:39:00 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [460/1200] - Loss: 53332.6992, MS-SSIM: 0.5985, AD: 0.8690\u001b[0m\n",
      "2025-09-30 10:39:00 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [480/1200] - Loss: 55432.4688, MS-SSIM: 0.5834, AD: 0.8890\u001b[0m\n",
      "2025-09-30 10:39:01 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [500/1200] - Loss: 56011.7695, MS-SSIM: 0.5744, AD: 0.8917\u001b[0m\n",
      "2025-09-30 10:39:01 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [520/1200] - Loss: 53933.5156, MS-SSIM: 0.5761, AD: 0.8806\u001b[0m\n",
      "2025-09-30 10:39:01 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [540/1200] - Loss: 51912.8867, MS-SSIM: 0.5897, AD: 0.8595\u001b[0m\n",
      "2025-09-30 10:39:01 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [560/1200] - Loss: 53932.6875, MS-SSIM: 0.5854, AD: 0.8748\u001b[0m\n",
      "2025-09-30 10:39:02 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [580/1200] - Loss: 54147.9727, MS-SSIM: 0.5865, AD: 0.8814\u001b[0m\n",
      "2025-09-30 10:39:02 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [600/1200] - Loss: 54057.9609, MS-SSIM: 0.5726, AD: 0.8838\u001b[0m\n",
      "2025-09-30 10:39:02 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [620/1200] - Loss: 54144.8945, MS-SSIM: 0.5986, AD: 0.8779\u001b[0m\n",
      "2025-09-30 10:39:02 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [640/1200] - Loss: 54305.9492, MS-SSIM: 0.5933, AD: 0.8789\u001b[0m\n",
      "2025-09-30 10:39:02 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [660/1200] - Loss: 54800.6875, MS-SSIM: 0.5826, AD: 0.8843\u001b[0m\n",
      "2025-09-30 10:39:03 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [680/1200] - Loss: 53610.6328, MS-SSIM: 0.5959, AD: 0.8760\u001b[0m\n",
      "2025-09-30 10:39:03 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [700/1200] - Loss: 51851.5781, MS-SSIM: 0.6008, AD: 0.8585\u001b[0m\n",
      "2025-09-30 10:39:03 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [720/1200] - Loss: 52425.4453, MS-SSIM: 0.5944, AD: 0.8639\u001b[0m\n",
      "2025-09-30 10:39:03 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [740/1200] - Loss: 56516.5078, MS-SSIM: 0.5838, AD: 0.9017\u001b[0m\n",
      "2025-09-30 10:39:04 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [760/1200] - Loss: 53144.5469, MS-SSIM: 0.5916, AD: 0.8690\u001b[0m\n",
      "2025-09-30 10:39:04 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [780/1200] - Loss: 52289.9648, MS-SSIM: 0.5943, AD: 0.8638\u001b[0m\n",
      "2025-09-30 10:39:04 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [800/1200] - Loss: 55812.6289, MS-SSIM: 0.5950, AD: 0.8978\u001b[0m\n",
      "2025-09-30 10:39:04 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [820/1200] - Loss: 54284.3008, MS-SSIM: 0.5795, AD: 0.8806\u001b[0m\n",
      "2025-09-30 10:39:05 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [840/1200] - Loss: 51414.2500, MS-SSIM: 0.6041, AD: 0.8539\u001b[0m\n",
      "2025-09-30 10:39:05 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [860/1200] - Loss: 53539.6523, MS-SSIM: 0.5721, AD: 0.8786\u001b[0m\n",
      "2025-09-30 10:39:05 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [880/1200] - Loss: 55149.7070, MS-SSIM: 0.5803, AD: 0.8843\u001b[0m\n",
      "2025-09-30 10:39:05 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [900/1200] - Loss: 54940.1562, MS-SSIM: 0.5836, AD: 0.8868\u001b[0m\n",
      "2025-09-30 10:39:06 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [920/1200] - Loss: 52322.3750, MS-SSIM: 0.5900, AD: 0.8621\u001b[0m\n",
      "2025-09-30 10:39:06 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [940/1200] - Loss: 50963.0000, MS-SSIM: 0.5953, AD: 0.8522\u001b[0m\n",
      "2025-09-30 10:39:06 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [960/1200] - Loss: 53865.8633, MS-SSIM: 0.5814, AD: 0.8740\u001b[0m\n",
      "2025-09-30 10:39:06 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [980/1200] - Loss: 55425.8242, MS-SSIM: 0.5825, AD: 0.8890\u001b[0m\n",
      "2025-09-30 10:39:07 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [1000/1200] - Loss: 55178.5352, MS-SSIM: 0.5834, AD: 0.8873\u001b[0m\n",
      "2025-09-30 10:39:07 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [1020/1200] - Loss: 54838.6367, MS-SSIM: 0.5841, AD: 0.8851\u001b[0m\n",
      "2025-09-30 10:39:07 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [1040/1200] - Loss: 53536.7383, MS-SSIM: 0.5923, AD: 0.8725\u001b[0m\n",
      "2025-09-30 10:39:07 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [1060/1200] - Loss: 54286.6484, MS-SSIM: 0.5922, AD: 0.8813\u001b[0m\n",
      "2025-09-30 10:39:08 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [1080/1200] - Loss: 51145.5156, MS-SSIM: 0.5957, AD: 0.8511\u001b[0m\n",
      "2025-09-30 10:39:08 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [1100/1200] - Loss: 53835.5781, MS-SSIM: 0.5805, AD: 0.8741\u001b[0m\n",
      "2025-09-30 10:39:08 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [1120/1200] - Loss: 54600.6992, MS-SSIM: 0.5821, AD: 0.8816\u001b[0m\n",
      "2025-09-30 10:39:08 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [1140/1200] - Loss: 54546.6172, MS-SSIM: 0.5802, AD: 0.8791\u001b[0m\n",
      "2025-09-30 10:39:09 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [1160/1200] - Loss: 52227.5469, MS-SSIM: 0.6055, AD: 0.8602\u001b[0m\n",
      "2025-09-30 10:39:09 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [1180/1200] - Loss: 53340.5195, MS-SSIM: 0.5890, AD: 0.8694\u001b[0m\n",
      "2025-09-30 10:39:09 \u001b[32mINFO     \u001b[0m 492157266.py: Validation [1200/1200] - Loss: 54504.6289, MS-SSIM: 0.5798, AD: 0.8812\u001b[0m\n",
      "2025-09-30 10:39:09 \u001b[32mINFO     \u001b[0m 492157266.py: Validation completed in 42.01s - Avg Loss: 54280.5612, Avg MS-SSIM: 0.5864, Avg AD: 0.8803\u001b[0m\n",
      "2025-09-30 10:39:09 \u001b[32mINFO     \u001b[0m 2906090605.py: üèÜ New best validation loss: 54280.561217\u001b[0m\n",
      "2025-09-30 10:39:09 \u001b[32mINFO     \u001b[0m 2906090605.py: üèÜ New best MS-SSIM: 0.586406\u001b[0m\n",
      "2025-09-30 10:39:09 \u001b[32mINFO     \u001b[0m 2906090605.py: üèÜ New best AD: 0.880317\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to output//best_val_loss.pth\n",
      "Model saved to output//best_ms_ssim.pth\n",
      "Model saved to output//best_ad.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 10:39:12 \u001b[32mINFO     \u001b[0m 2906090605.py: üìä Best Train Loss: 57414.4472 (epoch 5), Best Val Loss: 54280.5612 (epoch 5), Best MS-SSIM: 0.5864 (epoch 5), Best AD: 0.8803 (epoch 5)\u001b[0m\n",
      "2025-09-30 10:39:12 \u001b[32mINFO     \u001b[0m 2906090605.py: ============================================================\u001b[0m\n",
      "2025-09-30 10:39:17 \u001b[32mINFO     \u001b[0m 2906090605.py: [  6/100] [  10/600] Loss: 54103.1289 (avg: 53869.3555) Time: 0.38s (avg: 0.39s) LR: 5.01e-04\u001b[0m\n",
      "2025-09-30 10:39:21 \u001b[32mINFO     \u001b[0m 2906090605.py: [  6/100] [  20/600] Loss: 53314.7188 (avg: 54019.5053) Time: 0.39s (avg: 0.39s) LR: 5.01e-04\u001b[0m\n",
      "2025-09-30 10:39:25 \u001b[32mINFO     \u001b[0m 2906090605.py: [  6/100] [  30/600] Loss: 53631.5039 (avg: 53930.8203) Time: 0.38s (avg: 0.39s) LR: 5.01e-04\u001b[0m\n",
      "2025-09-30 10:39:29 \u001b[32mINFO     \u001b[0m 2906090605.py: [  6/100] [  40/600] Loss: 53775.8086 (avg: 53913.9609) Time: 0.39s (avg: 0.39s) LR: 5.01e-04\u001b[0m\n",
      "2025-09-30 10:39:32 \u001b[32mINFO     \u001b[0m 2906090605.py: [  6/100] [  50/600] Loss: 53603.5859 (avg: 53849.9024) Time: 0.38s (avg: 0.38s) LR: 5.01e-04\u001b[0m\n",
      "2025-09-30 10:39:36 \u001b[32mINFO     \u001b[0m 2906090605.py: [  6/100] [  60/600] Loss: 53272.4336 (avg: 53756.7751) Time: 0.39s (avg: 0.38s) LR: 5.01e-04\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def train():    \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(config.gpu_id)\n",
    "    if config.output_dir is None:\n",
    "        config.output_dir = 'output'\n",
    "    if config.restart_training:\n",
    "        shutil.rmtree(config.output_dir, ignore_errors=True)\n",
    "    if not os.path.exists(config.output_dir):\n",
    "        os.mkdir(config.output_dir)\n",
    "\n",
    "    # Initialize logging v√† metrics\n",
    "    log_file = os.path.join(config.output_dir, f'train_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n",
    "    logger = setup_logger(log_file)\n",
    "    metrics_tracker = TrainingMetrics(config.output_dir)\n",
    "    \n",
    "    # Log system info\n",
    "    logger.info(\"=== SYSTEM INFORMATION ===\")\n",
    "    logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "    logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"CUDA version: {torch.version.cuda}\")\n",
    "        logger.info(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "        logger.info(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    # Log training configuration\n",
    "    logger.info(\"=== TRAINING CONFIGURATION ===\")\n",
    "    logger.info(f\"Epochs: {config.epochs}\")\n",
    "    logger.info(f\"Batch size: {config.train_batch_size}\")\n",
    "    logger.info(f\"Accumulation steps: {getattr(config, 'accumulation_steps', 1)}\")\n",
    "    logger.info(f\"Learning rate: {config.lr}\")\n",
    "    logger.info(f\"GPU ID: {config.gpu_id}\")\n",
    "    logger.info(f\"Mixed Precision: {getattr(config, 'use_amp', True)}\")\n",
    "    logger.info(f\"Gradient Clipping: {getattr(config, 'grad_clip', 1.0)}\")\n",
    "    logger.info(f\"Eval interval: {getattr(config, 'eval_interval', 5)}\")\n",
    "    logger.info(\"==============================\")\n",
    "\n",
    "    # Set seeds\n",
    "    torch.manual_seed(config.seed)\n",
    "    if config.gpu_id is not None and torch.cuda.is_available():\n",
    "        logger.info(f'Training with GPU {config.gpu_id} and PyTorch {torch.__version__}')\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        torch.cuda.manual_seed(config.seed)\n",
    "        torch.cuda.manual_seed_all(config.seed)\n",
    "    else:\n",
    "        logger.info(f'Training with CPU and PyTorch {torch.__version__}')\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    # Data loading v·ªõi error handling\n",
    "    try:\n",
    "        train_data = ImageData(config.trainroot, transform=transforms.ToTensor(), t_transform=transforms.ToTensor())\n",
    "        train_loader = Data.DataLoader(\n",
    "            dataset=train_data, \n",
    "            batch_size=config.train_batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=int(config.workers),\n",
    "            pin_memory=True if torch.cuda.is_available() else False,\n",
    "            persistent_workers=True if int(config.workers) > 0 else False,\n",
    "            prefetch_factor=2  # S·ªë batch m√† m·ªói worker s·∫Ω preload (m·∫∑c ƒë·ªãnh l√† 2)\n",
    "        )\n",
    "        \n",
    "        test_data = ImageData(config.testroot, transform=transforms.ToTensor(), t_transform=transforms.ToTensor())\n",
    "        test_loader = Data.DataLoader(\n",
    "            dataset=test_data, \n",
    "            batch_size=1, \n",
    "            shuffle=False, \n",
    "            num_workers=3,\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Training samples: {len(train_data)}\")\n",
    "        logger.info(f\"Validation samples: {len(test_data)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    # Model setup\n",
    "    writer = SummaryWriter(config.output_dir)\n",
    "    net = TinyDocUnet(input_channels=3, n_classes=2)\n",
    "    net = net.to(device)\n",
    "\n",
    "    # Compile model\n",
    "    if getattr(config, 'use_compile', True):\n",
    "        net = torch.compile(net, options={\"triton.cudagraphs\": False}  # ‚Üê T·∫Øt CUDA Graphs\n",
    "    )\n",
    "\n",
    "    # Log model info\n",
    "    total_params = sum(p.numel() for p in net.parameters())\n",
    "    trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    logger.info(f\"Model parameters: {total_params:,} (trainable: {trainable_params:,})\")\n",
    "\n",
    "    criterion = DocUnetLoss(reduction='mean')\n",
    "    optimizer = torch.optim.AdamW(net.parameters(), lr=config.lr, weight_decay=1e-4)\n",
    "    \n",
    "    # Mixed precision setup\n",
    "    use_amp = getattr(config, 'use_amp', True) and torch.cuda.is_available()\n",
    "    scaler = GradScaler() if use_amp else None\n",
    "    grad_clip = getattr(config, 'grad_clip', 1.0)\n",
    "    \n",
    "    if use_amp:\n",
    "        logger.info(\"Using Automatic Mixed Precision (AMP)\")\n",
    "    if grad_clip > 0:\n",
    "        logger.info(f\"Using gradient clipping with max norm: {grad_clip}\")\n",
    "    \n",
    "    # Gradient accumulation\n",
    "    accumulation_steps = getattr(config, 'accumulation_steps', 1)\n",
    "    effective_batch_size = config.train_batch_size * accumulation_steps\n",
    "    logger.info(f\"Effective batch size: {effective_batch_size}\")\n",
    "\n",
    "    # Load checkpoint if exists\n",
    "    start_metrics = {}\n",
    "    if config.checkpoint != '' and not config.restart_training:\n",
    "        start_epoch, start_metrics = load_checkpoint(config.checkpoint, net, optimizer, scaler)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=start_epoch\n",
    "        )\n",
    "    else:\n",
    "        start_epoch = config.start_epoch\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=start_epoch - 1\n",
    "        )\n",
    "\n",
    "    all_step = len(train_loader)\n",
    "    global_step = start_epoch * all_step\n",
    "    epoch = 0\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"üöÄ Starting training...\")\n",
    "        training_start_time = time.time()\n",
    "        \n",
    "        for epoch in range(start_epoch, config.epochs):\n",
    "            net.train()\n",
    "            train_loss = 0.\n",
    "            accumulated_loss = 0.\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            # Statistics tracking\n",
    "            batch_times = []\n",
    "            losses = []\n",
    "            \n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "                batch_start_time = time.time()\n",
    "                \n",
    "                images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                \n",
    "                # Forward pass\n",
    "                if use_amp:\n",
    "                    with autocast(device_type='cuda'):\n",
    "                        _, y = net(images)\n",
    "                        loss = criterion(y, labels)\n",
    "                else:\n",
    "                    _, y = net(images)\n",
    "                    loss = criterion(y, labels)\n",
    "                \n",
    "                # Scale loss for accumulation\n",
    "                loss = loss / accumulation_steps\n",
    "                \n",
    "                # Backward pass\n",
    "                if use_amp:\n",
    "                    scaler.scale(loss).backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                \n",
    "                accumulated_loss += loss.item()\n",
    "                train_loss += loss.item() * accumulation_steps\n",
    "                losses.append(loss.item() * accumulation_steps)\n",
    "                \n",
    "                # Update weights\n",
    "                if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "                    if use_amp:\n",
    "                        if grad_clip > 0:\n",
    "                            scaler.unscale_(optimizer)\n",
    "                            grad_norm = torch.nn.utils.clip_grad_norm_(net.parameters(), grad_clip)\n",
    "                        \n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        if grad_clip > 0:\n",
    "                            grad_norm = torch.nn.utils.clip_grad_norm_(net.parameters(), grad_clip)\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Log accumulated loss\n",
    "                    writer.add_scalar('Train/accumulated_loss', accumulated_loss, global_step // accumulation_steps)\n",
    "                    \n",
    "                    # Log gradient norm\n",
    "                    if grad_clip > 0 and (global_step // accumulation_steps) % 50 == 0:\n",
    "                        writer.add_scalar('Train/grad_norm', grad_norm, global_step // accumulation_steps)\n",
    "                    \n",
    "                    accumulated_loss = 0.\n",
    "                \n",
    "                batch_time = time.time() - batch_start_time\n",
    "                batch_times.append(batch_time)\n",
    "                \n",
    "                # Periodic logging\n",
    "                if (i + 1) % config.display_interval == 0:\n",
    "                    current_lr = scheduler.get_last_lr()[0]\n",
    "                    avg_batch_time = np.mean(batch_times[-50:])  # Last 50 batches\n",
    "                    avg_loss = np.mean(losses[-50:])  # Last 50 batches\n",
    "                    \n",
    "                    logger.info(f'[{epoch + 1:3d}/{config.epochs}] [{i + 1:4d}/{all_step}] '\n",
    "                              f'Loss: {loss.item() * accumulation_steps:.4f} (avg: {avg_loss:.4f}) '\n",
    "                              f'Time: {batch_time:.2f}s (avg: {avg_batch_time:.2f}s) '\n",
    "                              f'LR: {current_lr:.2e}')\n",
    "                \n",
    "                # TensorBoard logging\n",
    "                writer.add_scalar('Train/batch_loss', loss.item() * accumulation_steps, global_step)\n",
    "                writer.add_scalar('Train/lr', scheduler.get_last_lr()[0], global_step)\n",
    "                writer.add_scalar('Train/batch_time', batch_time, global_step)\n",
    "                global_step += 1\n",
    "            \n",
    "            # Step scheduler\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Epoch statistics\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            avg_batch_time = np.mean(batch_times)\n",
    "            throughput = len(train_data) / epoch_time  # samples/second\n",
    "            \n",
    "            logger.info(f'=== EPOCH {epoch + 1:3d}/{config.epochs} SUMMARY ===')\n",
    "            logger.info(f'Train loss: {avg_train_loss:.6f}')\n",
    "            logger.info(f'Epoch time: {epoch_time:.2f}s')\n",
    "            logger.info(f'Avg batch time: {avg_batch_time:.3f}s')\n",
    "            logger.info(f'Throughput: {throughput:.1f} samples/sec')\n",
    "            logger.info(f'Learning rate: {current_lr:.2e}')\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_ms_ssim, val_ad = None, None, None\n",
    "            if (epoch + 1) % getattr(config, 'eval_interval', 5) == 0:\n",
    "                logger.info(\"üîç Running validation...\")\n",
    "                val_loss, val_ms_ssim, val_ad = validate_epoch(net, test_loader, criterion, device, logger, use_amp)\n",
    "                \n",
    "                # TensorBoard logging\n",
    "                writer.add_scalar('Val/Loss', val_loss, epoch)\n",
    "                writer.add_scalar('Val/MS-SSIM', val_ms_ssim, epoch)\n",
    "                writer.add_scalar('Val/AD', val_ad, epoch)\n",
    "                \n",
    "                # Save best models\n",
    "                is_best_val_loss = val_loss < metrics_tracker.best_metrics['best_val_loss']\n",
    "                is_best_ms_ssim = val_ms_ssim > metrics_tracker.best_metrics['best_ms_ssim']\n",
    "                is_best_ad = val_ad < metrics_tracker.best_metrics['best_ad']\n",
    "                \n",
    "                if is_best_val_loss:\n",
    "                    logger.info(f\"üèÜ New best validation loss: {val_loss:.6f}\")\n",
    "                    save_checkpoint(\n",
    "                        f'{config.output_dir}/best_val_loss.pth', \n",
    "                        net, optimizer, epoch + 1, scaler, \n",
    "                        {'train_loss': avg_train_loss, 'val_loss': val_loss, 'ms_ssim': val_ms_ssim, 'ad': val_ad}\n",
    "                    )\n",
    "                \n",
    "                if is_best_ms_ssim:\n",
    "                    logger.info(f\"üèÜ New best MS-SSIM: {val_ms_ssim:.6f}\")\n",
    "                    save_checkpoint(\n",
    "                        f'{config.output_dir}/best_ms_ssim.pth', \n",
    "                        net, optimizer, epoch + 1, scaler, \n",
    "                        {'train_loss': avg_train_loss, 'val_loss': val_loss, 'ms_ssim': val_ms_ssim, 'ad': val_ad}\n",
    "                    )\n",
    "                \n",
    "                if is_best_ad:\n",
    "                    logger.info(f\"üèÜ New best AD: {val_ad:.6f}\")\n",
    "                    save_checkpoint(\n",
    "                        f'{config.output_dir}/best_ad.pth', \n",
    "                        net, optimizer, epoch + 1, scaler,\n",
    "                        {'train_loss': avg_train_loss, 'val_loss': val_loss, 'ms_ssim': val_ms_ssim, 'ad': val_ad}\n",
    "                    )\n",
    "            \n",
    "            # Update metrics\n",
    "            metrics_tracker.update(\n",
    "                epoch=epoch + 1,\n",
    "                train_loss=avg_train_loss,\n",
    "                val_loss=val_loss,\n",
    "                learning_rate=current_lr,\n",
    "                epoch_time=epoch_time,\n",
    "                batch_time=avg_batch_time,\n",
    "                throughput=throughput,\n",
    "                val_ms_ssim=val_ms_ssim,\n",
    "                val_ad=val_ad\n",
    "            )\n",
    "            \n",
    "            # TensorBoard epoch summary\n",
    "            writer.add_scalar('Train/epoch_loss', avg_train_loss, epoch)\n",
    "            writer.add_scalar('Train/epoch_time', epoch_time, epoch)\n",
    "            writer.add_scalar('Train/throughput', throughput, epoch)\n",
    "            \n",
    "            # Save regular checkpoint\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                checkpoint_name = f'checkpoint_epoch_{epoch + 1:03d}.pth'\n",
    "                save_checkpoint(\n",
    "                    f'{config.output_dir}/{checkpoint_name}', \n",
    "                    net, optimizer, epoch + 1, scaler,\n",
    "                    {'train_loss': avg_train_loss, 'val_loss': val_loss, 'ms_ssim': val_ms_ssim, 'ad': val_ad}\n",
    "                )\n",
    "            \n",
    "            # Save metrics v√† create plots\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                metrics_tracker.save_metrics()\n",
    "                metrics_tracker.plot_training_curves()\n",
    "            \n",
    "            logger.info(f'üìä {metrics_tracker.get_best_info()}')\n",
    "            logger.info(\"=\" * 60)\n",
    "            \n",
    "            # Memory cleanup\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Training completed\n",
    "        total_training_time = time.time() - training_start_time\n",
    "        logger.info(\"üéâ TRAINING COMPLETED SUCCESSFULLY! üéâ\")\n",
    "        logger.info(f\"Total training time: {total_training_time / 3600:.2f} hours\")\n",
    "        logger.info(f\"Average time per epoch: {total_training_time / config.epochs:.2f}s\")\n",
    "        logger.info(f\"Final metrics: {metrics_tracker.get_best_info()}\")\n",
    "        \n",
    "        # Save final artifacts\n",
    "        metrics_tracker.save_metrics()\n",
    "        metrics_tracker.plot_training_curves()\n",
    "        save_checkpoint(f'{config.output_dir}/final_model.pth', net, optimizer, epoch + 1, scaler)\n",
    "        \n",
    "        writer.close()\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"‚ö†Ô∏è Training interrupted by user\")\n",
    "        save_checkpoint(f'{config.output_dir}/interrupted_model_epoch_{epoch + 1}.pth', net, optimizer, epoch + 1, scaler)\n",
    "        metrics_tracker.save_metrics()\n",
    "        metrics_tracker.plot_training_curves()\n",
    "        writer.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Training failed with error: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Full traceback:\\n{traceback.format_exc()}\")\n",
    "        save_checkpoint(f'{config.output_dir}/error_model_epoch_{epoch + 1}.pth', net, optimizer, epoch + 1, scaler)\n",
    "        metrics_tracker.save_metrics()\n",
    "        writer.close()\n",
    "        raise\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
